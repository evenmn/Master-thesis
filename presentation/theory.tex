\titleframe{Quantum Theory}

\note{Now we will give a breif introduction to the essential quantum theory.}

\mframe{The Schrödinger Equation}{}{
	\begin{empheq}[box={\mybluebox[5pt]}]{equation}
	\hat{\mathcal{H}}\Psi=E\Psi
	\end{empheq}
	\pause
	\vspace{0.2cm}
	\begin{center}
		{\large $\Downarrow$}
	\end{center}
	\vspace{0.3cm}
	\begin{equation}
	E=\frac{\int d\bs{X}\Psi^*(\bs{X})\hat{\mathcal{H}}\Psi(\bs{X})}{\int d\bs{X}\Psi^*(\bs{X})\Psi(\bs{X})}
	\end{equation}
	\note<1->{
		\begin{itemize}
			\item Describes the mechanics of all QM systems
			\item Stationary systems $\rightarrow$ Time-independent SE
			\item Linear algebra terms
			\item Configuration interaction
			\item One year on solving 
			\item Difficult to solve bco interactions between particles
		\end{itemize}
	}
}

\iffalse
\note{The Schrödinger equation describes the motion of any quantum mechanical system, and is the equation that I have spent one year solving. Since we will limit us to stationary systems only, the time-independent Schrödinger equation will be our focus. In linear algebra terms, it is an eigenvalue equation with the Hamilton operator, $\hat{\mathcal{H}}$, as a matrix and the wave function, $\Psi$, as the eigen function. $E$ is the energy, which is the eigenvalue.

\vspace{0.5cm}
The most natural way of solving this equation, is to simply express the Hamiltonian as a matrix and obtain the wave function and the energy from diagonalizing the matrix. This is known as configuration interaction. However, we can only do this for small systems, as it is very computational intensive. 
}

\note{Instead, we separate the equation with respect to the energy, and obtain a equation consisting of some integrals. This equation is hard to solve due to the electron-electron correlations. }
\fi

\mframe{The Variational Principle}{}{
	
	
	The variational principle serves as a way of finding the ground state energy. For an arbitrary trial wave function $\Psi_T(\bs{X})$, it states that the obtained energy is larger or equal to the ground state,
	\begin{equation}
	E_0\leq E=\frac{\int d\bs{X}\Psi_T^*(\bs{X})\hat{\mathcal{H}}\Psi_T(\bs{X})}{\int d\bs{X}\Psi_T^*(\bs{X})\Psi_T(\bs{X})}.
	\end{equation}
	Thus, by minimizing the obtained energy, $E$, we can estimate the ground state energy. 
}

\note{
	\begin{itemize}
		\item To obtain the ground state energy
		\item States $\rightarrow$ minimizing
	\end{itemize}
}

\mframe{Quantum Dots}{}{
	Circular quantum dots $\rightarrow$ electrons confined in a harmonic oscillator potential:
	\begin{equation} 		\hat{\mathcal{H}}=\sum_{i=1}^N\left[-\frac{1}{2}\nabla_i^2+\frac{1}{2}\omega^2|\bs{r}_i|^2+\sum_{j>i}^N\frac{1}{r_{ij}}\right].
	\end{equation}
	The number of electrons that give full shells are given by
	\begin{equation}
	N=2\binom{n+d}{d},
	\end{equation}
	which are the magic numbers. 
}

\note{
	\begin{itemize}
		\item Hamiltonian of the circular quantum dots consisting of electrons. In natural units.
		\item The magic numbers give the number of electron in each shell. Looked at closed-shell systems only
	\end{itemize}
}

\titleframe{Machine Learning Theory}

\note{Now over to the machine learning theory. We have already mentioned the artificial neural networks, and we will now look at how they actually work. }

\mframe{Feed-forward Neural Network (FNN)}{}{
	\begin{figure}
		\centering
		\begin{overprint}[9cm]
		\onslide<1>\input{../tikz/multilayer_perceptron_presentation1.tex}
		\onslide<2>\input{../tikz/multilayer_perceptron_presentation2.tex}
		\onslide<3>\input{../tikz/multilayer_perceptron_presentation3.tex}
		\onslide<4>\input{../tikz/multilayer_perceptron_presentation4.tex}
		\end{overprint}
	\end{figure}
	\hspace{1.45cm}
	\onslide<1-> $\bs{a}_0=\bs{x}$
	\hspace{.95cm}
	\onslide<2-> $\bs{a}_1=f_1(\bs{a}_0)$
	\hspace{1.35cm}
	\onslide<3-> $\bs{a}_2=f_2(\bs{a}_1)$
	\hspace{.35cm}
	\onslide<4-> $\tilde{\bs{y}}=f_3(\bs{a}_2)$
	\note<1->{
		\begin{itemize}
			\item FNNs are among the most popular neural networks
			\item Here a FNN
			\item Many different architectures
			\item Data set $\rightarrow$ propagating
			\item 
		\end{itemize}
	}
}

\mframe{Cost function}{}{
	\begin{itemize}
		\setlength\itemsep{3em}
		\item<1-> The cost function defines the error
		\item<2-> Mean square error (MSE): $$\mathcal{C}=\frac{1}{2}\sum_{i=1}^n(\bs{y}-\tilde{\bs{y}})^2.$$
		\item<3-> Attempt to minimize the cost function
	\end{itemize}
	\note<1->{
		\begin{itemize}
			\item To decide how good the model performs
			\item Continuous model $\rightarrow$ MSE
			\item Want the error to be small $\rightarrow$ minimize the cost function
		\end{itemize}
	}
}

\mframe{Optimization Algorithms}{}{
	\begin{itemize}
		\setlength\itemsep{3em}
		\item<1-> Minimize the cost function
		\item<2-> The gradient descent method: $$\theta^+=\theta-\frac{\partial\mathcal{C}}{\partial\theta}.$$
	\end{itemize}
	\begin{figure}
		\centering
		\begin{overprint}[7cm]
		\onslide<3>\input{../pgf/gradient_minimization1.tex}
		\onslide<4>\input{../pgf/gradient_minimization2.tex}
		\onslide<5>\input{../pgf/gradient_minimization3.tex}
		\end{overprint}
	\end{figure}
	\note<1->{
		\begin{itemize}
			\item For this, we use optimization algorithms
			\item Plenty of methods $\rightarrow$ tradeoff between simplicity and performance
			\item GD perhaps the simplest $\rightarrow$ move in the direction that minimizes the cost function
			\item We have used the ADAM optimizer, which is slightly more complex. Contains momentum
		\end{itemize}
	}
}

%\note{For the minimization, we use the optimization algorithms which basically find the minimum of any function. There are plenty of different algorithms, which have to tradeoff between simplicity and performance. Perhaps the most basic algorithm is gradient descent, which seeks the minimum based on the gradient. The function is then minimized with respect to the steepest slope, until we have found a minimum. }

\mframe{Find Appropriate Complexity}{}{
	\begin{figure}
		\centering
		\input{../tikz/traintest.tex}
	\end{figure}
}

\note{
	\begin{itemize}
		\item Different architectures with different complexities
		\item Spkit data set in training and test set
		\item Want to minimize test error $\rightarrow$ trial and error
	\end{itemize}
}

\mframe{Restricted Boltzmann Machines}{}{
	\begin{figure}
		\centering
		\input{../tikz/restricted_boltzmann_machine_presentation.tex}
	\end{figure}
	\pause
	\begin{equation}
	E(\bs{x},\bs{h})=-\sum_{i=1}^V\frac{(x_i-a_i)^2}{2\sigma_i^2}-\sum_{j=1}^Hh_jb_j-\sum_{i=1}^V\sum_{j=1}^H\frac{x_iw_{ij}h_j}{\sigma_i^2}
	\end{equation}
}

\note{
	\begin{itemize}
		\item What we have used in our work
		\item Energy based model
		\item Differs from FNN $\rightarrow$ obey unsupervised $\rightarrow$ no labeled data
		\item Finds the most likely configuration by minimizing the system energy
	\end{itemize}
}

\mframe{Probability Distribution}{}{
	The joint probability distribution is given by the Boltzmann distribution:
	
	\begin{equation}
	P(\bs{x},\bs{h})=\frac{1}{Z}\exp(-E(\bs{x},\bs{h})/kT).
	\end{equation}
	The marginal distribution of the visible units is given by
	\begin{equation}
	P(\bs{x})=\sum_{\{\bs{h}\}}P(\bs{x},\bs{h}).
	\end{equation}
}

\note{
	\begin{itemize}
		\item Named Boltzmann machine because of the joint probability distribution
		\item Find the marginal distribution of the visible units by integrating over all the hidden units
	\end{itemize}
}