\chapter{Machine Learning} \label{chp:machinelearning}
\epigraph{In the early 1990's we were working with machine learning all the time, but back then we called it pattern recognition and regression.}{Prof. Anne Solberg, UiO}
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.4]{Images/example.png}
	\caption{Caption}
\end{figure}

\iffalse
Parameter estimation is a large part of science, because we are often not able to measure things directly. For instance, when Millikan in ... performed his famous electron charge experiment or when we estimate Sun's mass.
\fi 

The use of the term \textit{machine learning} has exploded over the past years, and sometimes it sounds like it is a totally new field. However, the truth is that many of the methods are relatively old, where for instance \textit{linear regression} was known early in the 19th century. \cite{legendre_nouvelles_1805, gauss_theoria_1809} Those methods have just recently been taken under the machine learning umbrella, which is one of the reasons why the term is used more frequently than before. As the professor A.Solberg at the University of Oslo pointed out during one of her lectures, machine learning covers all methods where the goal is to minimize a loss function based on a set of parameters. This has made machine learning an extremely rich field. Unlike traditional algorithms, machine learning algorithms are not told what to do directly, but they use optimization tools to reproduce targets in the best way. As a consequence, we often do not know exactly what the algorithm does and why it behaves as is does, and because of this behavior, the processing is often called artificial intelligence. In our search for a technique to solve quantum mechanical problems where less physical intuition is needed, machine learning appears as a natural tool.

The increasing popularity should also without any doubt be attributed to the dramatically improvement of a majority of the machine learning algorithms. Perhaps the most important contribution to this came in 2012, when the deep convolution neural network (CNN) \textbf{AlexNet} managed to classify 1.2 millions images into 1000 classes with a remarkable top-5 test error rate of 15.3\%. \cite{krizhevsky_imagenet_2012} Today, the CNNs have been further improved, and they are even able to beat humans in recognizing images! (The history began from AlexNet) Also voice recognition algorithms have lately been revolutionized, thanks to recurrent neural networks (RNNs), and especially long short-term memory (LSTM) networks. Their ability to recognize sequential (time-dependent) data made the technology good enough for an entry to millions of peoples everyday-life through services such as \textbf{Google Translate} \cite{wu_googles_2016}, \textbf{Apple Siri} \cite{smith_ios_2016} and \textbf{Amazon Alexa} \cite{noauthor_bringing_nodate}. It is also interesting to see how machine learning has made computers eminent tacticians using recurrent neural networks. The \textbf{Google DeepMind} developed program \textbf{AlphaGo} demonstrated this by beating the 9-dan professional L. Sedol in the board game Go \cite{noauthor_alphago_nodate}, before an improved version, \textbf{AlphaZero}, beat the at that time highest rated chess computer, \textbf{StockFish} \cite{klein_mikeklein_googles_nodate}. Both these scenarios were unbelievable just a decade ago.

Even though all these branches are both exciting and promising, they will not be discussed further in this work, since they will simply not work for our purposes. The reason is that they initially require a data set with known outputs in order to be trained, they obey so-called \textit{supervised} learning. Instead, we rely on \textit{unsupervised} learning, which has the task of finding patterns in the data and is therefore not in need for known outputs. However, we will discuss some simpler supervised learning algorithms as an introduction and a motivation for the unsupervised learning section. But first, let us start with the fundamentals. 

\section{Statistical learning}
In machine learning, we want to fit a model to a data set. This is usually done by an initial guess on the model, and we thereafter train it to minimize the out-of-sample error. This error is a function of the \textit{variance} and \textit{bias}, where the former is a measure on how much the error fluctuates and the latter represents the the best our model could do if we had an infinite amount of training data. An useful illustration of this is found in figure \eqref{fig:bias_variance}.

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.6]{Images/bias_variance.png}
	\caption{Taken from \cite{mehta_high-bias_2019}.}
	\label{fig:bias_variance}
\end{figure}

The bias is reduced as a function of the complexity, but the variance tends to get higher as the complexity of the model is increased. For that reason, a very complex model is not guaranteed to provide a small error, but instead we need to find the trade-off between how expressive our model class is and how sensitive the fitted model is to sample fluctuations in the training data. This is known as the bias-variance trade-off, and is illustrated in figure \eqref{fig:bias_variance_tradeoff}.

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.6]{Images/bias_variance_tradeoff.png}
	\caption{Bias-variance trade-off. Taken from \cite{mehta_high-bias_2019}.}
	\label{fig:bias_variance_tradeoff}
\end{figure}
\
However, as Mehta et.al. notice, this is not necessarily a trade-off; we could both have a high variance and a high bias, and perhaps a more appropriate term would be bias-variance decomposition since the error can be decomposed into a bias part and a variance part. \cite{mehta_high-bias_2019} This decomposition is given by
\begin{equation}
E_{out}=bias^2+\sigma^2.
\end{equation}


\section{Supervised learning}
In supervised learning methods, we know the corresponding targets to the input data sets, which we use to train the model. This could for instance be linear regression, logistic regression or feed-forward neural networks.

Linear regression is perhaps the most intuitive example on this, where we want to find the line that fits some data points in the best possible way. In two dimensions, the $x$-coordinates are the inputs to the model and the $y$-coordinates are the targets. For instance, if we want to fit a second order polynomial,
\begin{equation}
f(x)=ax^2+bx+c,
\end{equation}
to a set of $n$ points $\{(x_1,y_1),(x_2,y_2),\hdots(x_n,y_n)\}$, one can set up a set of equations 
\begin{equation}
%\begin{vmatrix}
\mqty{
	\hat{y}_1&=&ax_1^2&+&bx_1&+&c\\
	\hat{y}_2&=&ax_2^2&+&bx_2&+&c\\
	\vdots&&\vdots&&\vdots&&\vdots\\
	\hat{y}_n&=&ax_n^2&+&bx_n&+&c
}
%\end{vmatrix}
\label{eq:lineareqs}
\end{equation}
where $\hat{y}_i$ is the $y$-value of the line at $x=x_i$. Our goal is to minimize the distance between $\hat{y}_i$ and $y_i$ by adjusting the parameters $a$, $b$ and $c$, which is usually done using the mean square error (MSE) as a measurement of the error. The loss function (also called the cost function) then reads
\begin{equation}
\mathcal{L}(a,b,c)=\sum_{i=1}^{n}\Big(y_i-(ax_i^2+bx_i+c)\Big)^2
\end{equation}
which can either be minimized by a matrix-vector product or an iterative minimization algorithm. Both these methods will be covered in the more general linear regression section below.

\subsection{Linear regression}
In linear regression, the dependent variable $y_i$ is a linear combination of the parameters, and for a dependent variable this can be written as
\begin{equation}
\hat{y}_i=\sum_jX_{ij}\beta_j
\label{eq:targets}
\end{equation}
where $\beta_j$'s are the unknown parameters to be found. In principle, $X_{ij}$ can be an arbitrary function of the arguments $x_i$, but in the polynomial case it can be simplified by setting $X_{ij} = x_i^j$. We will proceed dealing with $X_{ij}$ for general purposes.

The three most commonly used linear regression methods are \textit{ordinary least square} (OLS) regression, Ridge regression and Lasso regression, where the former has the loss function
\begin{empheq}[box={\mybluebox[5pt]}]{equation}
	\mathcal{L}(\bs{\beta})=\sum_{i=1}^{n}\Big(y_i-\beta_0-\sum_{j=1}^pX_{ij}\beta_j\Big)^2,\qquad\qquad\qquad\text{OLS}
\end{empheq}
which is minimized when
\begin{equation}
\bs{\beta}=(\hat{X}^T\hat{X})^{-1}\hat{X}^T\bs{y}.
\label{eq:ols}
\end{equation}

Quite often when we deal with large data sets, the design matrix contains singular values which give us problems with calculating $(\hat{X}^T\hat{X})^{-1}$. A way to avoid this is to introduce a penalty $\lambda$ to ensure that all the diagonal values are non-zero, which can be accomplished by adding a small value to all diagonal elements. This is the idea behind Ridge regression, which has the loss function
\begin{empheq}[box={\mybluebox[5pt]}]{equation}
	\mathcal{L}(\bs{\beta})=\sum_{i=1}^{n}\Big(y_i-\beta_0-\sum_{j=1}^pX_{ij}\beta_j\Big)^2+\lambda\sum_{j=1}^p\beta_j^2\qquad\text{Ridge}
\end{empheq}
and is minimized when
\begin{equation}
\bs{\beta}=(\hat{X}^T\hat{X}+\lambda I)^{-1}\hat{X}^T\bs{y}.
\end{equation}
Finally, we have Lasso regression with a loss function given by
\begin{empheq}[box={\mybluebox[5pt]}]{equation}
	\mathcal{L}(\bs{\beta})=\sum_{i=1}^{n}\Big(y_i-\beta_0-\sum_{j=1}^pX_{ij}\beta_j\Big)^2+\lambda\sum_{j=1}^p\beta_j\qquad\text{Lasso}
\end{empheq}
and without any expression for the optimized parameters. In order to optimize this loss function, we therefore need to use an iterative optimization algorithm. Such methods will later be used in the variational Monte-Carlo sampling, and some methods are therefore detailed in chapter \eqref{chp:optimization}. 

To illustrate how the iterative optimization works, we will go back to equation \eqref{eq:targets} and use OLS for simplicity reasons. Imagine that we have four points $\{(x_1,y_1),(x_2,y_2),(x_3,y_3),(x_4,y_4)\}$ that we want to fit to a curve of three parameters $\beta_1,\beta_2,\beta_3$. The simplest way to solve this interatively is to use \textbf{gradient descent} optimization, which goes as
\begin{equation}
\beta_k^+=\beta_k-\eta \frac{\partial\mathcal{L}(\bs{\beta})}{\partial\beta_k}
\end{equation}
with $\eta$ specifying how much the parameters should be changed for each iteration, known as the learning rate. 
The output $\hat{y}_i$ can be expressed by
\begin{equation}
\hat{y}_i=X_{i1}\beta_1+X_{i2}\beta_2+X_{i3}\beta_3
\end{equation}
and the network can be illustrated with $\beta_j$ as the input nodes, $\hat{y}_i$ as the output nodes and $X_{ij}$ as lines in between all nodes, as in figure ... 

ADD FIGURE

To go further, we will restrict  ourselves to a second order polynomial, setting $X_{i1}=1$, $X_{i1}=x_i$ and $X_{i1}=x_i^2$. The parameter update then yields
\begin{equation}
\beta_k^+=\beta_k+2\eta\sum_{i=1}^3(y_i-\beta_1-\beta_2x_i-\beta_3x_i^2)x_i^{k-1},
\end{equation}
which can be run iteratively until the parameters have converged. 

We will again stress that we go through this iterative method for illustration purposes only, it would be more efficient to apply equation \eqref{eq:ols}. However, the thinking above is similar to the approach we do when training a neural network. An important difference is that we here search for an estimate of the left-hand-side (LHS) \textit{nodes} $b_j$, while in a traditional feed-forward neural network (FNN) we want to find the \textit{weights}, the parameters that connect the nodes together (here $X_{ij}$). The principle with multiplying the LHS nodes by the weights to obtain the right-hand-side (RHS) nodes is though the same, and we can use the same minimization tools. An example that is even more related to neural networks is \textit{logistic regression}, which will be discussed in the next section. 

\subsection{Logistic regression}
In the previous section we presented how to fit a polynomial to a set of points using linear regression. In this chapter we will look at classification problems using logistic regression. Traditionally, the perceptron model was used for \textit{hard classification}, which sets the outputs directly to binary values. However, often we are interested in the probability of a given category, which means that we need a continuous \textit{activation function}. Logistic regression can, like linear regression, be considered as a function where weights are adjusted with the intention to minimize the error. An illustration of a simple perceptron is found in figure (\ref{fig:single_perceptron}).

\input{tikz/singlelayer_perceptron.tex}

Consider first a classification problem with two possible outcomes, for example the ultimatum: will I pass or fail the exam? In that case an output node is sufficient, where 0 is pass, 1 is fail and an intermediate value gives the probability that I will fail. In order to map the output to a number between 0 and 1, we use an activation function.

But, how does the model know if I will pass or fail? In order to get any useful information from the perceptron, we need to train it first. In our case, one could for instance use data from other students as input, and train the model as long as we know if they passed or failed. 

Now, what if we want to predict the grade instead of just pass/fail? In that case, we need a class for each grade one can get. A popular choice is to use \textit{one hot encoding}, which fires 

The very first step is to calculate the initial outputs (forward phase), where the weights usually are set to small random numbers. Then the error is calculated, and the weights are updated to minimize the error (backward phase). So far so good.

\subsubsection{Forward phase}\label{sec:forward}
Let us look at it from a mathematical perspective, and calculate the net output. The net output seen from an output node is simply the sum of all the "arrows" that point towards the node, see figure (\ref{fig:single_perceptron}), where each "arrow" is given by the left-hand node multiplied with its respective weight. For example, the contribution from input node 2 to the output node follows from $X_2\cdot w_{2}$, and the total net output to the output $O$ is therefore
\begin{empheq}[box={\mybluebox[5pt]}]{equation}
	net = \sum_{i=1}^{I} x_i\cdot w_i + b\cdot 1.
	\label{eq:forward}
\end{empheq}
Just some notation remarks: $x_i$ is the value of input node $i$ and $w_{i}$ is the weight which connects input $i$ to the output. $b$ is the bias weight, which we will discuss later.

You might wonder why we talk about the net output all the time, do we have other outputs? If we look at the network mathematically, what we talk about as the net output should be our only output. Anyway, it turns out to be convenient mapping the net output to a final output using an activation function, which is explained further in section \ref{sec:sigmoid1}. The activation function, $f$, takes in the net output and gives the output, 
\begin{equation}
out = f(net).
\end{equation}
If not everything is clear right now, it is fine. We will discuss the most important concepts before we dive into the maths.

\subsubsection{BIAS}
As mentioned above, we use biases when calculating the outputs. The nodes, with value $B$, are called the bias nodes, and the weights, $b$, are called the bias weights. But why do we need those? 

Suppose we have two inputs of value zero, and one output which should not be zero (this could for instance be a NOR gate). Without the bias, we will not be able to get any other output than zero, and in fact the network would struggle to find the right weights even if the output had been zero. 

The bias value $B$ does not really matter since the network will adjust the bias weights with respect to it, and is usually set to 1 and ignored in the calculations. [2]

\subsubsection{Learning rate}
In principle, the weights could be updated without adding any learning rate ($\eta=1$), but this can in practice be problematic. It is easy to imagine that the outputs can be fluctuating around the targets without decreasing the error, which is not ideal, and a learning rate can be added to avoid this. The downside is that with a low learning rate the network needs more training to obtain the correct results (and it takes more time), so we need to find a balance. 

\subsubsection{Loss function}\label{sec:loss_function}
The loss function is what defines the error, and in logistic regression the cross-entropy function is a naturally choice. [3] It reads
\begin{empheq}[box={\mybluebox[5pt]}]{equation}
	c(\boldsymbol{W}) = -\sum_{i=1}^n\Big[y_i\log f(\boldsymbol{x}_i^T\boldsymbol{W})+(1-y_i)\log[1-f(\boldsymbol{x}_i^T\boldsymbol{W})]\Big]
	\label{eq:cross_entropy}
\end{empheq}
where $\bs{W}$ contains all weights, included the bias weight ($\bs{W}\equiv[b,\bs{W}]$), and similarly does $\bs{x}$ include the bias node, which is 1; $\bs{x}\equiv[1,\bs{x}]$. Further, the $f(x)$ is the activation function discussed in next section.

The cross-entropy function is derived from likelyhood function, which famously reads
\begin{equation}
p(y|x)=\hat{y}^y\cdot(1-\hat{y})^{1-y}.
\end{equation}
Working in the log space, we can define a log likelyhood function
\begin{align}
	\log\Big[p(y|x)\Big]&=\log\Big[\hat{y}^y\cdot(1-\hat{y})^{1-y}\Big]\\
	&=y\log\hat{y}+(1-y)\log(1-\hat{y})
\end{align}
which gives the log of the probability of obtaining $y$ given $x$. We want this quantity to increase then the loss function is decreased, so we define our loss function as the negative log likelyhood function. [7]

Additionally, including a regularization parameter $\lambda$ inspired by Ridge regression is often convenient, such that the loss function is
\begin{equation}
c(\bs{W})^+=c(\bs{W})+\lambda||\bs{W}||_2^2.
\end{equation}
We will later study how this regularization affects the classification accuracy. 

\subsubsection{Activation function}\label{sec:sigmoid1}
Above, we were talking about the activation function, which is used to activate the net output. In binary models, this is often just a step function firing when the net output is above a threshold. For continuous outputs, the logistic function given by
\begin{empheq}[box={\mybluebox[5pt]}]{equation}
	f(x)=\frac{1}{1+e^{-x}}.
	\label{eq:logistic}
\end{empheq}
is usually used in logistic regression to return a probability instead of a binary value. This function has a simple derivative, which is advantageous when calculating a large number of derivatives. As shown in section \ref{sec:sigmoid_der}, the derivative is simply
\begin{equation}
\frac{df(x)}{dx}=x(1-x).
\label{eq:logistic_der}
\end{equation}

$tanh(x)$ is another popular activation function in logistic regression, which more or less holds the same properties as the logistic function. 

\subsubsection{Backward phase}
Now all the tools for finding the outputs are in place, and we can calculate the error. If the outputs are larger than the targets (which are the exact results), the weights need to be reduced, and if the error is large the weights need to be adjusted a lot. The weight adjustment can be done by any minimization method, and we will look at a couple of gradient methods. To illustrate the point, we will stick to the \textbf{gradient descent} (GD) method in the calculations, even though other methods will be used later. The principle of GD is easy: each weight is "moved" in the direction of steepest slope,
\begin{empheq}[box={\mybluebox[5pt]}]{equation}
	\bs{w}^+= \bs{w} - \eta\cdot\frac{\partial c(\boldsymbol{w})}{\partial \bs{w}},
	\label{eq:w_update}
\end{empheq}
where $\eta$ is the learning rate and $c(\bs{w})$ is the loss function. We use the chain rule to simplify the derivative
\begin{equation}
\frac{\partial c(\bs{w})}{\partial \bs{w}} =\frac{\partial c(\bs{w})}{\partial out} \cdot\frac{\partial out}{\partial net} \cdot\frac{\partial net}{\partial \bs{w}}
\end{equation}
where the first is the derivative of the loss function with respect to the output. For the cross-entropy function, this is
\begin{equation}
\frac{\partial c(\bs{w})}{\partial out}=-\frac{y}{out}+\frac{1-y}{1-out}.
\end{equation}
Further, the second derivative is the derivative of the activation function with respect to the output, which is given in \eqref{eq:logistic_der}
\begin{equation}
\frac{\partial out}{\partial net}=out(1-out).
\end{equation}
The latter derivative is the derivative of the net output with respect to the weights, which is simply
\begin{equation}
\frac{\partial net}{\partial \bs{w}}=\bs{x}.
\end{equation}

If we now recall that $out=f(\bs{x}^T\bs{w})$, we can write 
\begin{equation}
\frac{\partial c(\bs{w})}{\partial \bs{w}}=[f(\bs{x}^T\bs{w})-\bs{y}]\bs{x}
\end{equation}
and obtain a weight update algorithm
\begin{empheq}[box={\mybluebox[5pt]}]{align}
	\bs{w}^+= \bs{w} - \eta\cdot[f(\bs{x}^T\bs{w})-\bs{y}]^T\bs{x}.
\end{empheq}
where the bias weight is included implicitly in $\bs{w}$ and the same applies for $\bs{x}$.

\subsection{Neural network} \label{sec:neural_network}
If you have understood logistic regression, understanding a neural network should not be a difficult task. According to  \textbf{the universal approximation theorem}, a neural network with only one hidden layer can approximate any continuous function. [8] However, often multiple layers are used since this tends to give fewer nodes in total. 

In figure \eqref{fig:neural_network}, a two-layer neural network (one hidden layer) is illustrated. It has some similarities with the logistic regression model in figure \eqref{fig:single_perceptron}, but a hidden layer and multiple outputs are added. In addition, the output is no longer probabilities and can take any number, which means that we do not need to use the logistic function on the outputs anymore.

\input{tikz/multilayer_perceptron.tex}

Without a hidden layer, we have seen that the update of weights is quite straight forward. For a neural network consisting of multiple layers, the question is: how do we update the weights when we do not know the values of the hidden nodes? And how do we know which layer causing the error? This will be explained in section \ref{sec:backward}, where one of the most popular techniques for that is discussed. Before that we will generalize the forward phase presented in logistic regression.

\subsubsection{Forward phase}
In section \ref{sec:forward}, we saw how the output is found for a single perceptron. Since we only had one output node, the weights could be stored in an array. Generally, it is more practical to store the weights in matrices, since they will have indices related to both the node on left-hand side and the node on the right-hand side. For instance, the weight between input node $x_3$ and hidden node $h_5$ in figure \eqref{fig:neural_network} is usually labeled as $w_{35}$. Since we have two layers, we also need to denote which weight set it belongs to, which we will do by a superscript ($w_{35}\Rightarrow w_{35}^{(1)}$). In the same way, $\bs{W}^1$ is the matrix containing all $w_{ij}^{(1)}$, $\bs{x}$ is the vector containing all $x_i$'s and so on. We then find the net outputs at the hidden layer to be
\begin{empheq}{equation}
	net_{h,j} = \sum_{i=1}^{I} x_i\cdot w_{ij}^{(1)}=\bs{x}^T\bs{W}_j^{(1)}
	\label{eq:forward_hidden}
\end{empheq}
where the $\bs{x}$ and $\bs{W}^{(1)}$ again are understood to take the biases. This will be the case henceforth. The real output to the hidden nodes will be
\begin{equation}
h_j = f(net_{h,j}).
\end{equation}
Further, we need to find the net output to the output nodes, which is obviously just
\begin{empheq}{equation}
	net_{o,j} = \sum_{i=1}^{H} h_i\cdot w_{ij}^{(2)}=\bs{h}^T\bs{W}_j^{(2)}
	\label{eq:forward_output}
\end{empheq}
We can easily generalize this. Looking at the net output to a hidden layer $l$, we get
\begin{empheq}[box={\mybluebox[5pt]}]{equation}
	\bs{net}_{h_l} = \bs{h^{(l-1)}}^T\bs{W}^{(l)}.
	\label{eq:forward_general}
\end{empheq}

\subsubsection{Activation function}
Before 2012, the logistic, the tanh and the pur linear functions where the standard activation functions, but then Alex Krizhevsky published an article where he introduced a new activation function called \textit{Rectified Linear Units (ReLU)} which outperformed the classical activation functions. [4] The network he used is now known as AlexNet, and helped to revolutionize the field of computer vision. [5] After that, the ReLU activation function has been modified several times (avoiding zero derivative among others), and example of innovations are \textit{leaky ReLU} and \textit{Exponential Linear Unit (ELU)}. All those networks are linear for positive numbers, and small for negative numbers. Often, especially in the output layer, a straight linear function is used as well.

In figure (\ref{fig:activation_functions}), \textit{standard RELU, leaky RELU} and \textit{ELU} are plotted along with the logistic function.
\iffalse
\begin{figure} [H]
	\centering
	\subfloat[logistic]{{\includegraphics[width=8cm]{../plots/sigmoid.png} }}
	\subfloat[ReLU]{{\includegraphics[width=8cm]{../plots/ReLU.png} }}\\
	
	\subfloat[Leaky ReLU]{{\includegraphics[width=8cm]{../plots/LeakyReLU.png} }}%
	\subfloat[ELU]{{\includegraphics[width=8cm]{../plots/ELU.png} }}
	\caption{Some more or less popular activation functions}%
	\label{fig:activation_functions}%
\end{figure}
\fi

\subsubsection{Backward Propagation} \label{sec:backward}
Backward propagation is probably the most used technique for updating the weights, and is actually again based on equation (\ref{eq:w_update}). What differs, is the differentiation of the net input with respect to the weight, which gets more complex as we add more layers. For one hidden layer, we have two sets of weights, where the last layer is updated in a similar way as for a network without hidden layer, but the inputs are replaced with the values of the hidden nodes:
\begin{empheq}[box={\mybluebox[5pt]}]{align}
	w_{ij}^{(2)+}= w_{ij}^{(2)} - \eta\cdot[f(h_i^Tw_{ij})-y_j]^Th_i.
\end{empheq}
We recognize the first part as $\delta_{ok}$, such that
\begin{empheq}[box={\mybluebox[5pt]}]{equation}
	w_{ij}^{(1)+} = w_{ij}^{(1)} - \eta\cdot\sum_{k=1}^{O}\delta_{ok}\cdot w_{jk}^{(2)}\cdot f'(out_{hj})\cdot x_i
\end{empheq}
where we recall $\delta_{ok}$ as
\begin{equation*}
	\delta_{ok}=-(t_{ok}-out_{ok})\cdot f'(out_{ok}).
\end{equation*}
For more layers, the procedure is the same, but we keep on inserting the obtained outputs from various layers.

\subsubsection{Summary}
Since it will be quite a lot calculations, I will just express the results here, and move the calculations to Appendix B. The forward phase in a three-layer perceptron is
\begin{empheq}[box={\mybluebox[5pt]}]{align}
	net_{hi}&=\sum_jw_{ji}^{(1)}\cdot x_j\notag\\
	out_{hi}&=\text{f}(net_{hi})\notag\\
	\notag\\
	net_{ki}&=\sum_jw_{ji}^{(2)}\cdot out_{hj}\\
	out_{ki}&=\text{f}(net_{ki})\notag\\
	\notag\\
	net_{oi}&=\sum_jw_{ji}^{(3)}\cdot out_{kj}\notag\\
	out_{oi}&=\text{f}(net_{oi})\notag
\end{empheq}
which can easily be turned into vector form. The backward propagation follows from the two-layer example, and we get
\begin{empheq}[box={\mybluebox[5pt]}]{align}
	w_{ij}^{(3)}&=w_{ij}^{(3)}-\eta\cdot\delta_{oj}\cdot out_{ki}\notag\\
	\notag\\
	w_{ij}^{(2)}&=w_{ij}^{(2)}-\eta\sum_{k=1}^O\delta_{ok}\cdot w_{jk}^{(3)}\cdot f'(out_{kj})\cdot out_{hi}\notag\\
	\notag\\
	w_{ij}^{(1)}&=w_{ij}^{(1)}-\eta\sum_{k=1}^O\sum_{l=1}^K\delta_{ok}\cdot w_{lk}^{(3)}\cdot f'(out_{kl})\cdot w_{jl}^{(2)}f'(out_{hj})\cdot x_i\notag
\end{empheq}
where we again use the short hand 
\begin{equation*}
	\delta_{oj}=(t_j-out_{oj})\cdot f'(out_{oj}).
\end{equation*}
If we compare with the weight update formulas for the two-layer case, we recognize some obvious connections, and it is easy to imagine how we can construct a general weight update algorithm, no matter how many layers we have. 

Now over to the problem we want to solve using neural networks.


\section{Unsupervised Learning}
In unsupervised learning, a neural network is given the inputs only, and does not know what the output should look like. The task is then to find structures in the data, comparing data sets to each other and categorize the data sets with respect to their similarities and differences. 

We have previously seen how the parameters can be adjusted using the backward propagation algorithm, but it does not work when we do not have prior known targets. Instead, we need to rely on a set of probabilities, and we therefore need to look carefully at the statistical foundation before we move on to the unsupervised algorithms.

\subsection{Statistical foundation}
In this section, we will explain the general relation between the joint probability distribution of two variables $x$ and $y$, the marginal distributions and the conditional distributions. The expressions can either be set up with respect to the continuous space or the discrete space, and we will do the latter since we in practice will deal with discrete data sets. 

The joint probability of measure both $x$ and $y$ is given by
\begin{equation}
p(x,y)=p(x|y)p(y)=p(y|x)p(x)
\label{eq:jointprob}
\end{equation}
where $p(x|y)$ and $p(y|x)$ are the conditional distributions of $x$ and $y$ respectively. $p(x)$ and $p(y)$ are called the marginal probabilities for $x$ and $y$, and by reordering equation \eqref{eq:jointprob}, we obtain Bayes' theorem
\begin{equation}
p(x|y)=\frac{p(y|x)p(x)}{p(y)}
\end{equation}
where $p(x)$ is the \textit{prior} probability, $p(y|x)$ is the \textit{likelyhood} function and $p(x|y)$ is the \textit{posterior} probability. $p(y)$ can sometimes be found by a sum over the joint probability,
\begin{equation}
p(y)=\sum_i p(x_i,y) = \sum_i p(y|x_i)p(x_i),
\end{equation}
but often this summation is intractable. Different techniques require different approaches to this problem, and for our case we will use Markov chain Monte-Carlo methods to bypass it. More about that in section \eqref{chp:methods}. 

For supervised learning, the 
The parameters will always be updated such that the probability is maximized. For instance, 

Next challenge is that we do not have the posterior

Kullback-Leibler divergence gives a measure of how much information is loss when one goes from one probability distribution to another. 

\subsubsection{Marginal Distributions}

\subsubsection{Conditional Distributions}

\subsection{Boltzmann Machines}
Boltzmann Machines are based on the more primitive Hopfield network, where a system of nodes is set up which defines the system energy. Inspired by statistical mechanics, the probability of finding the system in a state of energy $E$ is given by the Boltzmann distribution,
\begin{equation}
P(\bs{s})=\frac{1}{Z}\exp(-E(\bs{s})/k_BT),
\label{eq:boltzmanndist}
\end{equation}
hence the name Boltzmann machines.  $\bs{s}$ includes all the nodes, $k_B$ is known as Boltzmann's constant and $T$ is the system temperature, but henceforth they both will be omitted by scaling $E'(\bs{s})=E(\bs{s})/k_BT$. $Z$ is known as the partition function, which is the sum over all possible probabilities.

In the most general form, all nodes are connected to all other nodes, that is an unrestricted Boltzmann machine, see figure \eqref{fig:boltzmann_machine} for an illustration. 

\input{tikz/boltzmann_machine.tex}

In the same manner as for a feed-forward neural network, we can directly multiply each node $s_i$ with all its respective inner weights $w_{ij}$ and then with the other nodes $s_j$. To obtain the total system energy, we also need to include the bias weights, i.e, multiply $s_i$ with $b_i$. This gives the energy
\begin{equation}
E(\bs{s})=- \sum_{i=1}^Ns_ib_i-\sum_{i=1}^N\sum_{j=i}^N s_iw_{ij}s_j 
\label{eq:unrestrictedboltzmannmachine}
\end{equation}
for a system of $N$ nodes, which is the so-called binary-binary network and the most basic architecture. During training, the weights are adjusted in order to maximize the probability...

\subsection{Restricted Boltzmann Machines}
When there is an unrestricted guy, a restricted guy must exist as well. What the term restricted means in this case, is that we ignore all the connections between nodes in the same layer, and keep only the inter-layer ones. In the same manner as in equation \eqref{eq:unrestrictedboltzmannmachine}, we can look at the linear case, where each node is multiplied with the corresponding weight, but now we need to distinguish between a visible node $x_i$ and a hidden node $h_j$. For the same reason, all the bias weights need to divided into a group connected to the visible nodes, $a_i$ and a group connected to the hidden nodes, $b_j$. The system energy then reads
\begin{equation}
E(\bs{x},\bs{h})=- \sum_{i=1}^Fx_ia_i- \sum_{j=1}^Hh_jb_j-\sum_{i=1}^F\sum_{j=i}^H x_iw_{ij}h_j 
\label{eq:binarybinary}
\end{equation}
which is called binary-binary units or Bernoulli-Bernoulli units. $F$ is the number of visible nodes and $H$ is number of hidden nodes. In figure \eqref{fig:restricted_boltzmann_machine}, a restricted Boltzmann machine with three visible nodes and three hidden nodes is illustrated.

\begin{figure} [H]
	\centering
	\input{tikz/restricted_boltzmann_machine.tex}
	\caption{Restricted Boltzmann machine. Black lines are the inter-layer connections, where for instance the line between $x_1$ and $h_1$ is related to the weight $w_{11}$. The blue lines are related to the input bias weights, and, for instance, the line going from the bias node to $x_3$ is called $a_3$. Similarly, the green lines are connections between the hidden nodes and the bias, and, for instance, the line going from the bias node to $h_3$ is called $b_3$.}
	\label{fig:restricted_boltzmann_machine}
\end{figure}

Until now we have discussed the linear models only, but as for feed-forward neural networks, we need non-linear models to solve non-linear problems. A natural next step is the Gaussian-binary units, which has a Gaussian mapping between the visible node bias and the visible nodes. The simplest such structure gives the following system energy:

\begin{equation}
E(\bs{x},\bs{h})= \sum_{i=1}^F\frac{(x_i-a_i)^2}{2\sigma_i^2} - \sum_{j=1}^Hh_jb_j-\sum_{i=1}^F\sum_{j=i}^H \frac{x_iw_{ij}h_j}{\sigma_i^2} 
\label{eq:gaussianbinary}
\end{equation}
where $\sigma_i$ is the width of the Gaussian distribution, which can be set to an arbitrary number. Inserting the energy expression into equation \eqref{eq:boltzmanndist}, we obtain the general expression 
\begin{equation}
P(\bs{x},\bs{h})=\exp\Big(-\sum_{i=1}^F\frac{(x_i-a_i)^2}{2\sigma^2}\Big)\prod_{j=1}^H\exp\Big(h_jb_j+\sum_{i=1}^F\frac{h_jw_{ij}x_i}{\sigma^2}\Big).
\label{eq:RBMWF1}
\end{equation}
which is the Gaussian-binary joint probability distribution. Generative sampling algorithms, as Gibbs' sampling, use this distribution directly, while other sampling tools, as Metropolis sampling, need the marginal distribution. Since the hidden nodes are binary, we just need to sum the joint probability distribution over $h=0$ and $h=1$ to find the marginal distributions. We obtain the expression
\begin{equation}
P(\bs{x})=\exp\Big(-\sum_{i=1}^F\frac{(x_i-a_i)^2}{2\sigma^2}\Big)\prod_{j=1}^H\bigg(1+\exp\Big(b_j+\sum_{i=1}^F\frac{w_{ij}x_i}{\sigma^2}\Big)\bigg).
\label{eq:RBMWF2}
\end{equation}

Similarly, the marginal distribution of the hidden nodes 

More about the different sampling tools can be found in chapter \ref{chp:systems}.

\subsection{Partly Restricted Boltzmann Machines}
One can also imagine a partly restricted architecture, where we have connections inwards the visible nodes, but not the hidden nodes. This is what we have decided to call a partly restricted Boltzmann machine. A such neural network with three visible nodes and three hidden nodes is illustrated in figure \eqref{fig:partly_restricted_boltzmann_machine}.

\begin{figure} [H]
	\centering
	\input{tikz/partly_restricted_boltzmann_machine.tex}
	\caption{Partly restricted Boltzmann machine. Black lines are inter-layer connections, where for instance the line between $x_1$ and $h_1$ is related to the weight $w_{11}$. The blue lines are related to the input bias weights, and, for instance, the line going from the bias node to $x_3$ is related to $a_3$. Similarly, the green lines are related to the hidden nodes bias weights, and, for instance, the line going from the bias node to $h_3$ is related to $b_3$. Finally, the red lines are the intra-layer connections related to the intra-layer weights. The weight between node $x_1$ and $x_2$ is called $c_{12}$. }
	\label{fig:partly_restricted_boltzmann_machine}
\end{figure}

Compared to a standard restricted Boltzmann machine, we get an extra term in the energy expression where the visible nodes are connected. It is easy to find that the expression should be

\begin{equation}
E(\bs{x},\bs{h})= \sum_{i=1}^F\frac{(x_i-a_i)^2}{2\sigma_i^2} - \sum_{i=1}^F\sum_{j>i}^Fx_ic_{ij}x_j- \sum_{j=1}^Hh_jb_j-\sum_{i=1}^F\sum_{j=i}^H \frac{x_iw_{ij}h_j}{\sigma_i^2} 
\label{eq:partlygaussianbinary}
\end{equation}
with $c_{ij}$ as the weights between the visible nodes. For the later calculations, we are interested in the marginal distribution only, which reads

\begin{equation}
p(\bs{x})=\exp\Big(-\sum_{i=1}^F\frac{(x_i-a_i)^2}{2\sigma^2}+\sum_{i=1}^F\sum_{j>i}^Fx_ic_{ij}x_j\Big)\prod_{j=1}^H\bigg(1+\exp\Big(b_j+\sum_{i=1}^F\frac{w_{ij}x_i}{\sigma^2}\Big)\bigg).
\label{eq:PRBMWF}
\end{equation}

\subsection{Deep Boltzmann Machines}
We can also construct deep Boltzmann machines, where we just stack single-layer Boltzmann machines. There are many ways to construct those networks, where the number of layers, unit types, number of nodes and the degree of restriction can be chosen as the constructor wants. The number of combinations is endless, but in order to make use of the dept, all the layer should have different configurations. Otherwise, the deep network can be reduced to a shallower network. In figure \eqref{fig:deep_restricted_boltzmann_machine} a restricted Boltzmann machine of two hidden layers is illustrated. We have chosen three hidden nodes in each layer, and three visible nodes. It should be trivial to imagine how the network can be expanded to more layers. 
\begin{figure} [H]
	\centering
	\input{tikz/deep_boltzmann_machine.tex}
	\caption{Deep restricted Boltzmann machine. Black lines the inter-layer connections, where for instance the line between $x_1$ and $h_1$ is related to the weight $w_{11}$. The blue lines are related to the input bias weights, and, for instance, the line going from the bias node to $x_3$ is related to $a_3$. Similarly, the green lines are related to the hidden nodes bias weights, and, for instance, the line going from the bias node to $h_3$ is related to $b_3$.}
	\label{fig:deep_restricted_boltzmann_machine}
\end{figure}

As the main focus so far has been restricted Boltzmann machines, also the deep networks will be assumed to be restricted, although both partly restricted and unrestricted can be constructed. The system energy of a deep restricted Boltzmann machine of $L$ layers can be expressed as
\begin{equation}
E(\bs{x},\bs{h})= \sum_{i=1}^F\frac{(x_i-a_i)^2}{2\sigma_i^2} - \sum_{l=1}^L\sum_{j=1}^Hh_j^lb_j^l-\sum_{l=1}^L\sum_{i=1}^F\sum_{j=i}^H \frac{x_iw_{ij}^lh_j^l}{\sigma_i^2}
\label{eq:deepgaussianbinary}
\end{equation}
which gives the marginal probability distribution
\begin{equation}
p(\bs{x})=\exp\Big(-\sum_{i=1}^F\frac{(x_i-a_i)^2}{2\sigma^2}\Big)\prod_{l=1}^L\prod_{j=1}^H\bigg(1+\exp\Big(b_j^l+\sum_{i=1}^F\frac{w_{ij}^lx_i}{\sigma^2}\Big)\bigg).
\label{eq:DRBMWF}
\end{equation}