\chapter{Many-body Quantum Mechanics} \label{chp:manybody}
\epigraph{I do not like it, and I am sorry I ever had anything to do with it.}{Erwin Schrödinger, \cite{noauthor_quantum_2005}}
\begin{figure}[H]
	\centering
	\captionsetup[subfigure]{labelformat=empty}
	\includegraphics[scale=3.0]{Images/art_quantum.jpg}
	\caption{The first photograph of a Hydrogen atom was captured by an ultra sensitive camera in 2013. One can actually see the probability distribution $|\Psi(\bs{r})|^2$ with the naked eye. Published in Phys. rev. lett. 110, 213001 (2013), \textit{Hydrogen atoms under magnification} \cite{stodolna_hydrogen_2013}.}
\end{figure}

In the previous chapter, the quantum mechanics of single particles was discussed. We presented the time-independent Schrödinger equation and from that we obtained the general expression from the energy,
\begin{equation}
E_n=\mel{\Psi_n(\bs{r})}{\hat{\mathcal{H}}}{\Psi_n(\bs{r})}
\label{eq:energy}
\end{equation}
where the Hamiltonian $\hat{\mathcal{H}}$ defines the system and $\Psi_n(\bs{r})$ defines the state $n$. What is wonderful, is that the same expression can be used for a system containing an arbitrary number of particles as long as the many-body wave function is defined. However, as noted before, the the wave function for large systems needs to store an immense amount of information and is therefore expensive or impossible to deal with. In this section we will first look at how the many-body Hamiltonian is set up, before we have an extensive discussion of how the many-body wave function is constructed.

\section{The electronic Hamiltonian} \label{sec:electronichamiltonian}
We have already seen what the one-body Hamiltonian looks like, and the many-body Hamiltonian is not very different. We recall that it can be split in a kinetic and a potential term,
\begin{equation}
\hat{H}=\hat{T}+\hat{V}
\end{equation}
where $\hat{T}$ is the kinetic energy and $\hat{V}$ is the potential energy. Nevertheless, as we study electrons, they are charged and will therefore interact with each other. For that reason, we need to add an interaction term to the Hamiltonian, which in general is included in the potential term $\hat{V}=V_{\text{ext}}+V_{\text{int}}$ with $V_{\text{ext}}$ as the external potential and $V_{\text{int}}$ as the interaction potential. In the same way as the nucleus potential, the interaction potential is given by Coulomb's law, for two electrons given by 
\begin{equation}
V_{\text{int}} =k_e\frac{e^2}{r_{12}}
\end{equation}
where $r_{12}$ is the distance between the electrons. For a general system containing $N$ electrons, the total Hamiltonian can therefore be expressed as 
\begin{equation}
\hat{\mathcal{H}}=-\sum_i^N\frac{\hbar^2}{2m_e}\nabla_i^2+\sum_i^{N}V_{\text{ext}}^i + \sum_i^N\sum_{j>i}^Nk_e\frac{e^2}{r_{ij}}
\label{eq:ElectronicHamiltonian}
\end{equation}
which is the farthest we can go without specifying the external potential $V_{\text{ext}}^i$. $r_{ij}$ is the relative distance between particle $i$ and $j$, defined by $r_{ij}\equiv|\bs{r}_i-\bs{r}_j|$. From now on, we will use atomic units setting $\hbar=m_e=k_e=e=1$, see appendix \ref{app:units}.

By putting this Hamiltonian into equation \eqref{eq:energy}, the integral can be split in three parts,
\begin{equation}
\begin{aligned}
E_n&=-\sum_{i=1}^N\bigg[\mel{\Psi_n(\bs{r})}{\nabla_i^2}{\Psi_n(\bs{r})}
+\mel{\Psi_n(\bs{r})}{V_{\text{ext}}^i}{\Psi_n(\bs{r})}
+\sum_{j>i}^N\mel{\Psi_n(\bs{r})}{\frac{1}{r_{ij}}}{\Psi_n(\bs{r})}\bigg]
\end{aligned}
\end{equation}
where the two former ones are the one-body integrals, or \textit{matrix elements}, which we in many cases easily can solve. However, the last term is very tricky 

\section{The wave function} \label{sec:wavefunction}
By the first postulate of quantum mechanics presented in section, the wave function contains all the information specifying the state of the system. This means that all observable in classical mechanics can also be estimated from the wave function, which makes finding the wave function our aim. We will in this section discuss the symmetry properties of the wave function for bosonic and fermionic systems, and see how the wave function can be set up as a Slater determinant in order to meet these properties. Also Jastrow factors will be touched, as they are used in quantum Monte Carlo methods to handle the correlations. With that in mind, we approximating the wave function by a \textit{trial wave function},
\begin{equation}
\Psi_T(\bs{r};\bs{\theta})=|\hat{D}(\bs{r};\bs{\theta})|J(\bs{r};\bs{\theta}).
\end{equation}
which is an educated guess on the form of the wave function. 
Here the first part, $|\hat{D}(\bs{r};\bs{\theta})|$, is the Slater determinant and $J(\bs{r};\bs{\theta})$ is a Jastrow factor with $\theta$ as some variational parameters. Other quantum many-body methods, such as full configuration interaction and coupled cluster use a linear expansion of Slater determinants to approximate the wave function, while the Hartree-Fock method relies on one single Slater determinant (without any Jastrow factor). Nevertheless, as we will focus on quantum Monte Carlo methods, this chapter will be tailored to the method. 

The trial wave function needs to satisfy some requirements in order to be used in the variational principle, and we thus need to make an educated guess on the wave function where the requirements are fulfilled. The requirements are the following:

\begin{enumerate}
	\item \textbf{Normalizability:} The wave function needs to be normalizable in order to make physical sense. The total probability should always be 1, and a wave function that cannot be normalized will not have a finite total probability. The consequence is that the wave function goes to zero when the positions get large, $\Psi(x\rightarrow\pm\infty)\rightarrow 0$. 
	
	\item \textbf{Cusp condition:} The cusp condition (also called the Kato theorem) states that the wave function should have a cusp where the potential explodes. An example on this is when charged particles come close to each other. %\change{Extend this, this is actually wrong}
	\todo[inline]{This is wrong}
	
	\item \textbf{Symmetry and anti-symmetry:} The wave function needs to be either symmetric or anti-symmetric under exchange of two coordinates, dependent on whether the particles are fermions or bosons. This is the statement of the sixth postulate, which will be further explained in the next section.
\end{enumerate}

\subsection{Anti-symmetry and the Pauli principle} \label{sec:symmetry}
Symmetry and anti-symmetry are central concepts in quantum mechanics, and often one can use symmetry arguments to simplify expressions and calculations. In this section we will look at the symmetry and anti-symmetry properties of the wave function and discuss how it can be used to define a wave function in the next section.

Assume that we have a permutation operator $\hat{P}$ which exchanges two coordinates in the wave function,
\begin{equation}
\hat{P}(i\rightarrow j)\Psi_n(\bs{r}_1,\hdots,\bs{r}_i,\hdots,\bs{r}_j,\hdots,\bs{r}_M)=p\Psi_n(\bs{r}_1,\hdots,\bs{r}_j,\hdots,\bs{r}_i,\hdots,\bs{r}_M),
\end{equation}
where $p$ is just a factor which comes from the transformation. If we again apply the $\hat{P}$ operator, we should switch the same coordinates back, and we expect to end up with the initial wave function. For that reason, $p$ must be either +1 or -1. \footnote{Actually, in two-dimensional systems we have a third possibility which gives an \textit{anyon}. The theory on this was developed by J.M. Leinaas and J. Myrheim during the 1970's \cite{leinaas_one_1977}.}

The particles that have an anti-symmetric wave function under exchange of two coordinates are called fermions, named after Enrico Fermi, and as discussed before they have half-integer spin. On the other hand, the particles that have a symmetric wave function under exchange of two coordinates are called bosons, named after Satyendra Nath Bose, and have integer spin. A consequence of the anti-symmetric wave function, is that two identical fermion cannot occupy the same state at the same time, known as the Pauli principle. This means that identical fermions even in the ground state spread over multiple states, and in the next section we will see how this principle is baked into the wave function through a Slater determinant. 

\subsection{The Slater determinant} \label{sec:slater}
For a system of many particles we can define a total wave function, which is a composition of all the single particle wave functions (SPF) and contains all the information about the system as the first postulate requires. For fermions, we need to combine the SPFs such that the Pauli principle is fulfilled at all times, which can be accomplished by a determinant. 

Consider a system of two identical fermions with SPFs $\psi_1(\bs{r},\sigma)$ and $\psi_2(\bs{r},\sigma)$ at position and spin $\boldsymbol{r}_1,\sigma_1$ and $\boldsymbol{r}_2,\sigma_2$ respectively. The way we define the wavefunction of the system is then
\begin{equation}
\begin{aligned}
\Psi(\bs{r}_1,\bs{r}_2, \sigma_1,\sigma_2)&=\frac{1}{\sqrt{2}}
\begin{vmatrix}
\psi_1(\boldsymbol{r}_1,\sigma_1) & \psi_2(\boldsymbol{r}_1,\sigma_2)\\
\psi_1(\boldsymbol{r}_2,\sigma_1) & \psi_2(\boldsymbol{r}_2,\sigma_2)
\end{vmatrix}\\
&=\frac{1}{\sqrt{2}}\Big[\psi_1(\boldsymbol{r}_1,\sigma_1)\psi_2(\boldsymbol{r}_2,\sigma_2)-\psi_2(\boldsymbol{r}_1,\sigma_2)\psi_1(\boldsymbol{r}_2,\sigma_1)\Big],
\end{aligned}
\end{equation}
which is set to zero if the particles are at the same position and have the same spin. If the particles, on the other hand, have different spins, they are allowed to be at the same position at the same time and the determinant will not cancel. For larger systems, the Slater determinant is constructed in the same way as above, and any pair of identical particles located in the same state will cause the determinant to collapse. A Slater determinant containing $N$ electrons reads
\begin{equation}
\Psi(\bs{r},\bs{\sigma})=\frac{1}{\sqrt{N!}}
\begin{vmatrix}
\psi_1(\boldsymbol{r}_1,\sigma_1) & \psi_2(\boldsymbol{r}_1,\sigma_2) & \hdots & \psi_N(\boldsymbol{r}_1,\sigma_N)\\
\psi_1(\boldsymbol{r}_2,\sigma_1) & \psi_2(\boldsymbol{r}_2,\sigma_2) & \hdots & \psi_N(\boldsymbol{r}_2,\sigma_N)\\
\vdots & \vdots & \ddots & \vdots \\
\psi_1(\boldsymbol{r}_N,\sigma_1) & \psi_2(\boldsymbol{r}_N,\sigma_2) & \hdots & \psi_N(\boldsymbol{r}_N,\sigma_N)
\end{vmatrix}
\end{equation}
where the $\psi(\bs{r},\sigma)$ is the tensor product between the radial part $\psi(\bs{r})$ and the spin part $\chi(\sigma)$,
\begin{equation}
\psi(\bs{r},\sigma)=\phi(\bs{r})\otimes\chi(\sigma).
\end{equation}
In the rest of this thesis, we will consequently use $\phi$ as the radial part of the single particle functions and $\chi$ as the spin part. In section \ref{sec:slaterdeterminant}, it is shown that the Slater determinant can be split in a spin-up part and a spin-down part such that the spin-dependency $\chi(\sigma)$ can be omitted. for that reason, we can define a basis set consisting of the spatial part only, discussed in the next section. 

\subsection{Basis set} \label{subsec:basisset}
To go further, we need to define a basis set, $\{\phi_1(\bs{r}),\phi_2(\bs{r}),\hdots\phi_N(\bs{r})\}$ which should be chosen carefully based on the system. For a few systems, we know the exact basis of the non-interacting case, and it is thus a natural basis to use in the Slater determinant. For other systems, the choice of basis might depend on the situation, where we typically need to weigh computational time against accuracy. Concrete examples on both cases will be presented in chapter \eqref{chp:systems}.

Often, one will see that the basis is optimized by the Hartree-Fock method. Using this basis in a single Slater determinant, we obtain the Hartree-Fock energy which sometimes is quite accurate. To get an even better energy estimate, we need to add more Slater determinants, which is the task of the post Hartree-Fock methods.

\subsection{Jastrow factors} \label{sec:jastrow}
From electrostatics we know that identical, charged particles will repel each other. This means that the probability of finding two particles close to each other should be low, which needs to be baked into the wave function. One way to do this is to simply multiply the wave function with the distance between the particles; the smaller distance the lower probability. However, since we are going to work in the logarithmic space, dealing with exponential function will be much easier. This is the main idea behind the simple Jastrow factor.

\subsubsection{Simple Jastrow}
The simple Jastrow factor is just an exponential function with the sum over all particle distances. In addition, each distance $r_{ij}$ is weighted by a parameter $\beta_{ij}$, and the factor becomes
\begin{equation}
J(\bs{r}; \bs{\beta}) = \exp\left(\sum_{i=1}^N\sum_{j>i}^N{\beta_{ij}r_{ij}}\right).
\label{eq:SimpleJastrow}
\end{equation}
All the $\beta_{ij}$ are free variational parameters, which are expected to be symmetric since the distance matrix is symmetric.

One problem with this Jastrow factor, is that it does not create the cusp around each particle correctly. Basically, the Jastrow factor increases faster than it should when a particle is moved away from another. To solve this, we need to introduce a more complex Jastrow factor, the Padé-Jastrow.

\subsubsection{Padé-Jastrow}
The Padé-Jastrow factor is closely related to the simple Jastrow above, but a denominator is added to make the cusp correctly. It reads
\begin{equation}
J(\bs{r};\beta) = \exp\bigg(\sum_{i=1}^N\sum_{j>i}^N\frac{a_{ij}r_{ij}}{1+\beta r_{ij}}\bigg).
\label{eq:PadeJastrow}
\end{equation}
where $\beta$ is a variational parameter. In addition, the fractions are multiplied with constants $a_{ij}$ which depend on the particles $i$ and $j$ in the following way:
\begin{equation}
\label{eq:ajastrow}
a_{ij}=
\begin{cases} 
e^2/(D+1) & \text{if $i,j$ are particles of same spin} \\
e^2/(D-1) & \text{if $i,j$ are particles of opposite spin},
\end{cases}
\end{equation}
for dimensions $D\in[2,3]$ where $e$ is the elementary charge. We will later use natural and atomic units, and set $e=1$, which for two dimensions gives $a_{ij}=1/3$ (same spin) or $a_{ij}=1$ (opposite spin) and for three dimensions $a_{ij}=1/4$ (same spin) and $a_{ij}=1/2$ (opposite spin) \cite{hogberget_quantum_2013,mariadason_quantum_2018}.

This Jastrow factor is known to give accurate results for fermions and bosons because it gives the correct cusp condition, and it is the one we gonna use in the standard variational Monte-Carlo simulations.

\section{Electron density}
In quantum many-body computations, the electron density is frequently calculated, and there are several reasons for that. Firstly, the electron density can be found experimentally, such that the calculations can be benchmarked. Secondly, the electron density is very informative, since information about all particles can be gathered in one plot.

The $P$-body electron density can be found by integrating over all particles but $P$, 
\begin{equation}
\label{eq:electron_density}
\rho_i(\bs{r})=\int_{-\infty}^{\infty}d\bs{r}_P\hdots d\bs{r}_N |\Psi(\bs{r}_1,\hdots \bs{r}_N)|^2.
\end{equation}
where $P<N$.

\subsection{One-body Density}
The one-body density is the most applied electron density, and is sometimes simply referred to as the electron density. For the two particle case, the one-body density gives the probability of finding one particle at a relative distance $r$ to the other. For more particles, the one-body density gives the probability of finding the mass center of the remaining particles at a relative distance $r$ from one of the particles.

The one-body density integral can be solved by Monte-Carlo integration. We then divide the space into bins of equal sizes at different radii and count the number of particles in each bin throughout the sampling. In practice, one often divide the space into bins where the radii are uniformly distributed, i.e, $r_i=i\cdot r_0$, see figure \eqref{fig:onebody}. In that case, one needs to divide the number of particles in each bin by its volume afterwards in order to get a correct distribution. In two dimensions, the area of bin $i$ is
\begin{equation}
A_i=(2i+1)\pi d^2
\end{equation}
and in three dimensions the volume of bin $i$ is
\begin{equation}
V_i=4(i(i+1)+1/3)\pi d^3.
\end{equation}
where $d$ is the radial width of a bin. 

\begin{figure}
	\centering
	\label{fig:onebody}
	\input{tikz/onebody_bins.tex}
	\caption{This figure is meant to illustrate how the one-body density is calculated using Monte-Carlo integration. One divides the space into $n$ bins (here 5), and count the number of particles in each bin throughout the sampling. Afterwards, the bins need to be normalized.}
\end{figure}

\begin{figure}
	\centering
	\label{fig:twobody}
	\input{tikz/twobody_bins.tex}
	\caption{This figure is meant to illustrate how the two-body density is calculated using Monte-Carlo integration. One divides the space into $n$ bins (here 5), and store the position of a pair of particles in a matrix throughout the sampling. Afterwards, the bins need to be normalized.}
\end{figure}

\subsection{Two-body Density}
The two-body density gives the probability of finding a pair of electrons at coordinates $\bs{r}_1$ and $\bs{r}_2$. 

For the one-body density, we integrate over all the particles but one, which corresponds to counting number of particles in each bin when doing Monte-Carlo integration. For the two-body density, we integrate over all particles but a \textit{particle pair}, which means that we need to find the position of each particle pair in order to solve the integral by Monte-Carlo integration. See figure \eqref{fig:twobody} for an illustration of this Monte-Carlo integration.

\subsection{Wigner Crystals} \label{sec:wigner}
A Wigner crystal is a solid phase where electrons form triangular lattices to minimize the potential energy. The phenomenon occurs only when the potential energy dominates the kinetic energy, since the electrons then are almost "at rest". 

To minimize the potential energy, the distances between the electrons should be maximized. To achieve this, the electrons form the triangular lattice shape, not so unlike Norwegians on the metro trying to maximize the distance to anyone else. For that reason, the electron density should approach discrete radial values, and the phenomenon should be observable in one-body density plots.


