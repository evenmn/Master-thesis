\section{What I have done} \label{sec:wihd}
As mentioned before, our wavefunction is inspired by machine learning in the sense that ... Neural networks ... Boltzmann machine ... \\
JOINT PROBABILITY\\

\iffalse
\subsubsection{Restricted Boltzmann Machines} \label{sec:RBM}
There are plenty of ways to put together a neural network, as one can modify the number of nodes and layers, the biases, and also which nodes that are allowed to communicate. In this project, we will be using the so-called restricted Boltzmann machine (RBM). The reason why it is named "restrictive" is that there are no connections between nodes in the same layer, but every node in the previous layer is connected to all the nodes in the next layer. The RBM can learn to draw samples from a probability distribution, which is just what we want in our project. In addition, we want to use a Gaussian-Binary RBM (see figure \ref{fig:RBM}) where the hidden nodes have binary values, while the visible nodes of the particles can take continuous values, as they are the positions of the particles. 

\begin{figure}
\centering
\begin{tikzpicture}[scale=2]
    % \draw ;
    \draw[fill=black!30, rounded corners] (-0.2, -0.2) rectangle (3.2, 0.2) {};
    \draw[fill=black!30, rounded corners] (0.3, 0.8) rectangle (2.7, 1.2) {};

    \node (v1)[neuron] at (0, 0) {$x_1$};
    \node (v2)[neuron] at (1, 0) {$x_2$};
    \node (v3)[neuron] at (2, 0) {$x_3$};
    \node (v4)[neuron] at (3, 0) {$x_4$};
    \node[right=0.1cm of v4] (v) {$\boldsymbol{X} = {\cal N}(\boldsymbol{X};\boldsymbol{a} + \boldsymbol{W} \boldsymbol{h}, \sigma^2)^4$};
    \node[learned,below=0.1cm of v1] (bv1) {$a_{x_1}$};
    \node[learned,below=0.1cm of v2] (bv2) {$a_{x_2}$};
    \node[learned,below=0.1cm of v3] (bv3) {$a_{x_3}$};
    \node[learned,below=0.1cm of v4] (bv4) {$a_{x_4}$};

    \node (h1)[neuron] at (0.5, 1) {$h_1$};
    \node (h2)[neuron] at (1.5, 1) {$h_2$};
    \node (h3)[neuron] at (2.5, 1) {$h_3$};
    \node[right=0.1cm of h3] (h) {$\boldsymbol{h} \in \{0, 1\}^3$};
    \node[learned,above=0.1cm of h1] (bh1) {$b_{h_1}$};
    \node[learned,above=0.1cm of h2] (bh2) {$b_{h_2}$};
    \node[learned,above=0.1cm of h3] (bh3) {$b_{h_3}$};

    \node[learned] (W) at (3.5, 0.5) {$W \in \mathbb{R}^{3 \times 4}$};

    \draw[learned,stateTransition] (v1) -- (h1) node [midway,above=-0.06cm,sloped] {$w_{1,1}$};
    \draw[stateTransition] (v1) -- (h2) node [midway,above=-0.06cm,sloped] {};
    \draw[stateTransition] (v1) -- (h3) node [midway,above=-0.06cm,sloped] {};

    \draw[stateTransition] (v2) -- (h1) node [midway,above=-0.06cm,sloped] {};
    \draw[stateTransition] (v2) -- (h2) node [midway,above=-0.06cm,sloped] {};
    \draw[stateTransition] (v2) -- (h3) node [midway,above=-0.06cm,sloped] {};

    \draw[stateTransition] (v3) -- (h1) node [midway,above=-0.06cm,sloped] {};
    \draw[stateTransition] (v3) -- (h2) node [midway,above=-0.06cm,sloped] {};
    \draw[stateTransition] (v3) -- (h3) node [midway,above=-0.06cm,sloped] {};

    \draw[stateTransition] (v4) -- (h1) node [midway,above=-0.06cm,sloped] {};
    \draw[stateTransition] (v4) -- (h2) node [midway,above=-0.06cm,sloped] {};
    \draw[learned,stateTransition] (v4) -- (h3) node [midway,above=-0.06cm,sloped] {$w_{4,3}$};

\end{tikzpicture}
\caption{An example of a Gaussian-Binary RBM with four visible nodes ($\boldsymbol{X}$) and three hidden nodes ($\boldsymbol{h}$), where $\boldsymbol{W}$ are the weights, and $\boldsymbol{a}$ and $\boldsymbol{b}$ are the bias weights. }
\label{fig:RBM}
\end{figure}

The joint probability distribution $F$ is known from statistical mechanics:
\begin{equation}
\label{eq:F_rbm}
F(\boldsymbol{X},\boldsymbol{H})=\frac{1}{Z}e^{-\beta E(\boldsymbol{X},\boldsymbol{H})}
\end{equation}
where we set $\beta=1/kT=1$ and $Z$ is the partition function, which can be ignored since it will vanish anyway. 

The system energy of a Gaussian-Binary RBM is given by:
\begin{equation}
E(\boldsymbol{X},\boldsymbol{H})=\sum_{i=1}^{M}\frac{(X_i-a_i)^2}{2\sigma_i^2}-\sum_{j=1}^Nb_jH_j-\sum_{i,j=1}^{M,N}\frac{X_iW_{ij}H_j}{\sigma_i^2}
\end{equation}
% \quad \cite{Hinton}

=========================================================================================
Not sure how much of the things above I can use.
\fi

For a quantum harmonic oscillator system, we know that the hermite functions are the true eigenfunctions. In two dimensions, they go as
\begin{equation}
\phi_{n_{x1},n_{x2}}(x_1,x_2)=H_{n_{x1}}(x_1)H_{n_{x2}}(x_2)\exp\Big(-\frac{x_1^2+x_2^2}{2}\Big)
\end{equation}
where $H_n(x)$ is the hermite polynomial of $n$'th order. We are going heavily inspired by this when setting up our SPFs, but in spirit of machine learning, we need to add variational variables:

\begin{equation}
\phi_{n_{x1},n_{x2}}(x_1,x_2,a_1,a_2)=H_{n_{x1}}(x_1-a_1)H_{n_{x2}}(x_2-a_2)\exp\Big(-\frac{(x_1-a_1)^2+(x_2-a_2)^2}{2\sigma^2}\Big)
\end{equation}
Notice that we subtract a variable $a_i$ from every position $x_i$, and that we also have added another variable $\sigma$. Since $x_i$ always is related to $a_i$, we introduce shorthand notation $x_i-a_i\equiv xa_i$. We then set up a slater determinant, and as described in section \ref{sec:slater} we can split it up in a spin-up part and a spin-down part:

\begin{equation}
D^{\uparrow}=
\begin{vmatrix} 
\phi_{0,0}(xa_1,xa_2)&\phi_{0,0}(xa_3,xa_4)&\phi_{0,0}(xa_5,xa_6)&\hdots \\
\phi_{1,0}(xa_1,xa_2)&\phi_{1,0}(xa_3,xa_4)&\phi_{1,0}(xa_5,xa_6)&\hdots \\
\phi_{0,1}(xa_1,xa_2)&\phi_{0,1}(xa_3,xa_4)&\phi_{0,1}(xa_5,xa_6)&\hdots \\
\vdots&\vdots&\vdots&\ddots
\end{vmatrix}
\end{equation}
SIMILARLY FOR $D^{\downarrow}$. When inserting the SPFs, one can observe that the exponential part can be factorized out, and we and up with determinants that consist of Hermite polynomials only:

\begin{equation}
\label{eq:wf_form}
\Psi_T(\vec{x},\vec{a},\vec{b},\hat{W},\sigma)=\frac{1}{Z}\detup'\detdn' e^{\sum_i^M \frac{(xa_i)^2}{2\sigma^2}} \prod_j^N (1+ e^{b_j + \sum_iM \frac{X_i W_{ij}}{\sigma^2}})
\end{equation}

With 

\begin{equation}
\detup'=
\begin{vmatrix} 
H_{0}(xa_1)H_{0}(xa_2)&H_{0}(xa_3)H_{0}(xa_4)&H_{0}(xa_5)H_{0}(xa_6)&\hdots \\
H_{1}(xa_1)H_{0}(xa_2)&H_{1}(xa_3)H_{0}(xa_4)&H_{1}(xa_5)H_{0}(xa_6)&\hdots \\
H_{0}(xa_1)H_{1}(xa_2)&H_{0}(xa_3)H_{1}(xa_4)&H_{0}(xa_5)H_{1}(xa_6)&\hdots \\
\vdots&\vdots&\vdots&\ddots
\end{vmatrix}
\end{equation}

\subsubsection{6 fermion example}
The theory above might be a bit abstract, so let's turn to an example: 6 electrons in two dimensions. 
\begin{equation}
\detup'=
\begin{vmatrix} 
H_{0}(xa_1)H_{0}(xa_2)&H_{0}(xa_3)H_{0}(xa_4)&H_{0}(xa_5)H_{0}(xa_6)\\
H_{1}(xa_1)H_{0}(xa_2)&H_{1}(xa_3)H_{0}(xa_4)&H_{1}(xa_5)H_{0}(xa_6)\\
H_{0}(xa_1)H_{1}(xa_2)&H_{0}(xa_3)H_{1}(xa_4)&H_{0}(xa_5)H_{1}(xa_6)
\end{vmatrix}
\end{equation}

\begin{equation}
\detdn'=
\begin{vmatrix} 
H_{0}(xa_7)H_{0}(xa_8)&H_{0}(xa_9)H_{0}(xa_{10})&H_{0}(xa_{11})H_{0}(xa_{12})\\
H_{1}(xa_7)H_{0}(xa_8)&H_{1}(xa_9)H_{0}(xa_{10})&H_{1}(xa_{11})H_{0}(xa_{12})\\
H_{0}(xa_7)H_{1}(xa_8)&H_{0}(xa_9)H_{1}(xa_{10})&H_{0}(xa_{11})H_{1}(xa_{12})
\end{vmatrix}
\end{equation}

Observe that we have particles in the two lowest energy levels only => Hermite polynomials of 1'st and 2'nd order:

\begin{align*}
H_0(x)&=1\\
H_1(x)&=2x
\end{align*}

which gives 

\begin{equation}
\detup'=
\begin{vmatrix} 
1&1&1\\
2xa_1&2xa_3&2xa_5\\
2xa_2&2xa_4&2xa_6
\end{vmatrix}
\end{equation}

\begin{equation}
\detdn'=
\begin{vmatrix} 
1&1&1\\
2xa_7&2xa_8&2xa_9\\
2xa_{10}&2xa_{11}&2xa_{12}
\end{vmatrix}
\end{equation}