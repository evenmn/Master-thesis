\chapter{Restricted Boltzmann Machines} \label{chp:restricted}
\epigraph{Available energy is the main object at stake in the struggle for existence and the evolution of the world.}{Ludwig Boltzmann, \cite{rajasekar_ludwig_2006}}
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.25]{Images/example.png}
	\caption{Caption}
\end{figure}

\section{Unsupervised learning} \label{sec:unsupervised}
In unsupervised learning, a neural network is given the inputs only, and does not know what the output should look like. The task is then to find structures in the data, comparing data sets to each other and categorize the data sets with respect to their similarities and differences. 

We have previously seen how the weights in supervised learning can be adjusted using the backward propagation algorithm, but it does not work when we do not have prior known targets. Instead, the weights are controlled by a set of probabilities, and we let the cost function be defined by the log likelihood function. This is known as Bayesian statistics, and will be presented in the next section.

\subsection{Statistical foundation} \label{sec:bayes}
In this section, we will use Bayesian statistics to exploit the link between some data $\bs{x}$, called the \textit{hypothesis}, and some other data $\bs{y}$, called the evidence.  We will first do it in a general way, before we link it to machine learning in the next section.

Bayesian statistics appear in many field of science, as it is a basic and often useful probability theory. It is based on Bayes' theorem, which gives rise to some marginal and conditional distributions. The expressions can either be set up in the continuous space or the discrete space, but here we will stick to the latter as we in practice will deal with discrete data. 

We start expressing the joint probability distribution of measuring both $\bs{x}$ and $\bs{y}$ using the general relation,
\begin{equation}
P(\bs{x},\bs{y})=P(\bs{x}|\bs{y})P(\bs{y})=P(\bs{y}|\bs{x})P(\bs{x}),
\label{eq:jointprob}
\end{equation}
which basically states that the probability of observing $\bs{x}$ \textit{and} $\bs{y}$ is just the probability of observing $\bs{x}$ multiplied with the probability of observing $\bs{y}$ given $\bs{x}$. 
$p(\bs{x}|\bs{y})$ is the conditional distribution of $\bs{x}$ and gives the probability of $\bs{x}$ given that $\bs{y}$ is true. The opposite applies for $P(\bs{y}|\bs{x})$. $P(\bs{x})$ and $P(\bs{y})$ are called the marginal probabilities for $\bs{x}$ and $\bs{y}$, and by reordering equation \eqref{eq:jointprob}, we obtain Bayes' theorem
\begin{equation}
P(\bs{x}|\bs{y})=\frac{P(\bs{y}|\bs{x})P(\bs{x})}{P(\bs{y})}.
\end{equation}
The marginal probability of $\bs{y}$, $P(\bs{y})$, is given by the sum over all the possible joint probabilities when $\bs{y}$ is fixed,
\begin{equation}
P(\bs{y})=\sum_i P(x_i,\bs{y}) = \sum_i P(\bs{y}|x_i)P(x_i),
\end{equation}
and from this we observe that Bayes' theorem gives us the \textit{posterior} probability, $P(\bs{x}|\bs{y})$, given the \textit{prior} probability, $P(\bs{x})$, and the \textit{likelihood}, $P(\bs{y}|\bs{x})$, seen from
\begin{equation}
P(\bs{x}|\bs{y})=\frac{P(\bs{y}|\bs{x})P(\bs{x})}{\sum_i P(\bs{y}|x_i)P(x_i)}.
\end{equation}
However, the summation gets extremely expensive quickly, and is intractable even for small systems. This was a big problem for a long time, but with the advent of powerful computers, algorithms like Markov chain Monte-Carlo can be used to estimate the posterior without knowing the \textit{normalization constant}, $P(\bs{y})$. More about that in chapter \ref{chp:methods}. 

In the section on supervised learning, the cost function was an important concept, and so is the case in unsupervised learning. But how do we define a cost function when we do not have any targets? We find the answer by revealing the similarities between the logistic regression and the Bayesian statistics. In logistic regression, we find the probability that a system is in a particular state, and define the cost function as the log-likelihood. We can do the same in unsupervised learning, and define the cost function as
\begin{equation}
\mathcal{C}(\bs{y})=\ln\prod_{i=1}^lP(\bs{x}_i|\bs{y})=\sum_{i=1}^l\ln P(\bs{x}_i|\bs{y})
\end{equation}
which is the log-likelihood. Maximizing the likelihood is the same as maximizing the log-likelihood, which again corresponds to minimizing the distance between the unknown distribution $Q$ underlying $\bs{x}$ and the distribution $P$ of the Markov random field $\bs{y}$. This distance is expressed in terms of the Kullback-Leibler divergence (KL divergence), which for a finite state space $\Omega$ is given by
\begin{equation}
\text{KL}(Q||P)=\sum_{\bs{x}\in\Omega}Q(\bs{x})\frac{Q(\bs{x})}{P(\bs{x})}.
\end{equation}
The KL divergence is a measure of the difference between two \textit{probability density functions} (PDFs), and is zero for two identical PDFs. The divergence is often called a distance, but that is an unsatisfying description as it is non-symmetric ($\text{KL}(Q||P))\neq\text{KL}(P||Q)$) in general. 

To proceed further, we will introduce latent variables in form of hidden units. Suppose we want to model an $m$-dimensional unknown probability distribution $Q$. Typically, not all the variables $\bs{s}$ are observed components, they can also be latent variables. If we split $\bs{s}$ into \textit{visible} variables $\bs{x}$ and hidden variables $\bs{h}$, and under the assumption that $\bs{x}$ and $\bs{h}$ are variables in an energy function $E(\bs{x},\bs{h})$, we can express the joint probability as the Boltzmann distribution
\begin{equation}
P(\bs{x},\bs{h})=\frac{\exp(-E(\bs{x},\bs{h}))}{Z}
\end{equation}
where $Z$ is the partition function, which is the sum of the probability of all possible states, which was already introduced in equation \eqref{eq:partition}. We have ignored the factor $k_BT$ by setting it to 1. Where the visible units correspond to components of an observation, the hidden units introduce the system to more degrees of freedom. This allows us to describe complex distributions over the visible variables by means of simple conditional distributions. \cite{fischer_training_2014} Those conditional distributions will be described later, but let us first take a look at the marginal distributions.

\subsubsection{Marginal distributions}
We have already used the term marginal distribution, which means that we get rid of a set of variables by integrating the joint probability over all of them. The marginal probability of $\bs{x}$ is given by
\begin{equation}
P(\bs{x})=\sum_{\bs{h}}P(\bs{x},\bs{h})=\frac{1}{Z}\sum_{\bs{h}}\exp(-E(\bs{x},\bs{h})).
\end{equation}
The sum over the $\bs{h}$ vector is just a short-hand notation where we sum over all the possible values of all the variables in $\bs{h}$. Further, the marginal probability of $\bs{h}$ is expressed similarly, with
\begin{equation}
P(\bs{h})=\sum_{\bs{x}}P(\bs{x},\bs{h})=\frac{1}{Z}\sum_{\bs{x}}\exp(-E(\bs{x},\bs{h})).
\end{equation}
$p(\bs{x})$ is important as it gives the probability of a particular set of visible units $\bs{x}$, while $p(\bs{h})$ will not be used in the same scope in this work. 

\subsubsection{Conditional distributions}
The conditional distributions can be found from Bayes' theorem, and read
\begin{equation}
P(\bs{h}|\bs{x})=\frac{P(\bs{x},\bs{h})}{P(\bs{x})}=\frac{\exp(-E(\bs{x},\bs{h}))}{\sum_{\bs{h}}\exp(-E(\bs{x},\bs{h}))}
\end{equation}
and
\begin{equation}
P(\bs{x}|\bs{h})=\frac{P(\bs{x},\bs{h})}{P(\bs{h})}=\frac{\exp(-E(\bs{x},\bs{h}))}{\sum_{\bs{x}}\exp(-E(\bs{x},\bs{h}))}.
\end{equation}
The conditional probabilities are especially important in Gibbs sampling, where we want to update the $\bs{x}$'s given a $\bs{h}$ and vice versa. 

\subsubsection{Maximum log-likelihood estimate}
Now suppose that the energy function also is a function of some parameters $\bs{\theta}$. We have already expressed the log-likelihood function, 
\begin{equation}
\ln P(\bs{x}|\bs{\theta})=\ln\bigg[\frac{1}{Z}\sum_{\bs{h}}\exp(-E(\bs{x},\bs{h}))\bigg]=\ln\sum_{\bs{h}}\exp(-E(\bs{x},\bs{h}))-\ln\sum_{\bs{x},\bs{h}}\exp(-E(\bs{x},\bs{h}))
\end{equation}
and by maximizing this we find the maximum log-likelihood estimate. This estimate is important in neural networks since we always seek to maximize the likelihood in the training process. The function is maximized when 
\begin{equation}
\begin{aligned}
\frac{\partial\ln P(\bs{x}|\bs{\theta})}{\partial\bs{\theta}}&=\frac{\partial}{\partial\bs{\theta}}\bigg(\ln\sum_{\bs{h}}\exp(-E(\bs{x},\bs{h}))\bigg)-\frac{\partial}{\partial\bs{\theta}}\bigg(\ln\sum_{\bs{x},\bs{h}}\exp(-E(\bs{x},\bs{h}))\bigg)\\
&=-\sum_{\bs{h}}P(\bs{h}|\bs{x})\frac{\partial E(\bs{x},\bs{h})}{\partial\bs{\theta}}+\sum_{\bs{x},\bs{h}}P(\bs{x},\bs{h})\frac{\partial E(\bs{x},\bs{h})}{\partial\bs{\theta}}=0.
\end{aligned}
\end{equation}

Similar to LASSO regression and neural networks, we cannot find a closed-form expression for this, and we need to solve it iteratively. 

\subsection{Boltzmann Machines}
Boltzmann Machines are energy-based, generative neural networks based on the more primitive Hopfield network. They were invented in 1985 by Geoffrey Hinton \cite{ackley_learning_1985}, often referred to as "The Godfather of Deep Learning"\footnote{Hinton's contribution to machine learning can hardly be overstated. He was co-author of the paper popularizing the backpropagation algorithm, \cite{rumelhart_learning_1986} supervisor of Alex Krizhevsky who designed AlexNet \cite{krizhevsky_imagenet_2012} and the main author of the paper introducing the regularization technique \textit{dropout} \cite{hinton_improving_2012}.}, and the network is named after the Boltzmann distribution.

A Boltzmann machine consists of a set of units where a node is connected to all other units through weights, similar to the neural network already presented. It is also common to add bias units. In figure \eqref{fig:boltzmann_machine}, a simple Boltzmann machine consisting of 6 units and 1 bias node is illustrated. 

\begin{figure}
	\centering
	\input{tikz/boltzmann_machine.tex}
	\caption{Unrestricted Boltzmann machine. Black lines are connections between all the units, where for instance the line between $s_1$ and $s_6$ is related to the weight $w_{16}$. The blue lines are related to the bias weights, and, for instance, the line going from the bias node to $s_3$ is related to $b_3$.}
	\label{fig:boltzmann_machine}
\end{figure}

By multiplying each node with all the other units and the weight connecting them, one obtains the system energy. For the simplest case, the energy reads
\begin{equation}
E(\bs{s})=- \sum_{i=1}^Ns_ib_i-\sum_{i=1}^N\sum_{j=i}^N s_iw_{ij}s_j 
\label{eq:unrestrictedboltzmannmachine}
\end{equation}
where $\bs{s}$ are the units and $w_{ij}$ is the weight between node $s_i$ and $s_j$. The bias node is fixed to 1, as always, and the weight between the bias node and the node $s_i$ is denoted by $b_i$. In its most simple form, the units can only take binary values, and we therefore call it a binary-unit Boltzmann machine. Also other architectures are possible. 

The energy formula is identical to the system energy of Hopfield networks, but what distinguish a Boltzmann machine from a Hopfield network is that the units are \textit{stochastic}. This means that their values are randomly determined, introducing some randomness to the system. Also the energy of an Ising model takes the same form as equation \eqref{eq:unrestrictedboltzmannmachine}

You might already have foreseen the next step, which is to use the Boltzmann distribution to define the probability of finding the system in a particular state $E(\bs{s};\bs{w},\bs{b})$, as discussed in the previous section. The probability distribution function (PDF) is then given by
\begin{equation}
P(\bs{s})=\frac{1}{Z}\exp(-E(\bs{s})),
\label{eq:boltzmanndist}
\end{equation}
where $Z$ again is the partition function. The PDF contains weights, which can be adjusted to change the distribution. In a supervised scheme, one can update the parameters in order to minimize the Kullback-Leibler divergence to a prior known distribution and in that manner reproduce the known distribution. In unsupervised learning, we cannot do this, but we can hope that a reasonable distribution is obtained by minimizing the system energy.

A Boltzmann machine is also a Markov random field, as the stochastic processes satisfy the Markov property. Loosely speaking, this means that all the probabilities of going from one state to another are known, making it possible to predict future of the process based solely on its present state. It is also determined by a "memorylessness", meaning that the next state of the system depends only on the current state and not on the sequence of events that preceded it. \cite{fischer_training_2014} Markov chains is an important part of the sampling methods that will be discussed later. 

\subsection{Restricted Boltzmann Machines} \label{sec:RBM}
When there is an unrestricted guy, a restricted guy must exist as well. What the term restricted means in this case, is that we ignore all the connections between units in the same layer, and keep only the inter-layer ones. Only the units in the first layer are the observable, while the units in the next layer are latent or hidden. In the same manner as in equation \eqref{eq:unrestrictedboltzmannmachine}, we can look at the linear case, where each node is multiplied with the corresponding weight, but now we need to distinguish between a visible node $x_i$ and a hidden node $h_j$. For the same reason, all the bias weights need to be divided into a group connected to the visible units, $a_i$ and a group connected to the hidden units, $b_j$. The system energy then reads
\begin{equation}
E(\bs{x},\bs{h})=- \sum_{i=1}^Fx_ia_i- \sum_{j=1}^Hh_jb_j-\sum_{i=1}^F\sum_{j=i}^H x_iw_{ij}h_j 
\label{eq:binarybinary}
\end{equation}
which is called binary-binary units or Bernoulli-Bernoulli units. $F$ is the number of visible units and $H$ is number of hidden units. In figure \eqref{fig:restricted_boltzmann_machine}, a restricted Boltzmann machine with three visible units and three hidden units is illustrated.

\begin{figure}
	\centering
	\input{tikz/restricted_boltzmann_machine.tex}
	\caption{Restricted Boltzmann machine. Black lines are the inter-layer connections, where for instance the line between $x_1$ and $h_1$ is related to the weight $w_{11}$. The blue lines are related to the input bias weights, and, for instance, the line going from the bias node to $x_3$ is called $a_3$. Similarly, the green lines are connections between the hidden units and the bias, and, for instance, the line going from the bias node to $h_3$ is called $b_3$.}
	\label{fig:restricted_boltzmann_machine}
\end{figure}

\subsubsection{Gaussian-binary units}
Until now we have discussed the linear models only, but as for feed-forward neural networks, we need non-linear models to solve non-linear problems. A natural next step is the Gaussian-binary units, which has a Gaussian mapping between the visible node bias and the visible units. The simplest such structure gives the following system energy:

\begin{equation}
E(\bs{x},\bs{h})= \sum_{i=1}^F\frac{(x_i-a_i)^2}{2\sigma_i^2} - \sum_{j=1}^Hh_jb_j-\sum_{i=1}^F\sum_{j=i}^H \frac{x_iw_{ij}h_j}{\sigma_i^2} 
\label{eq:gaussianbinary}
\end{equation}
where $\sigma_i$ is the width of the Gaussian distribution, which can be set to an arbitrary number. Inserting the energy expression into equation \eqref{eq:boltzmanndist}, we obtain the general expression
\begin{equation}
P(\bs{x},\bs{h})\propto\exp\Big(-\sum_{i=1}^F\frac{(x_i-a_i)^2}{2\sigma^2}\Big)\prod_{j=1}^H\exp\Big(h_jb_j+\sum_{i=1}^F\frac{h_jw_{ij}x_i}{\sigma^2}\Big)
\label{eq:RBMWF1}
\end{equation}
which is the Gaussian-binary joint probability distribution. Generative sampling algorithms, as Gibbs' sampling, use this distribution directly, while other sampling tools, as Metropolis sampling, need the marginal distribution. Since the hidden units are binary, we just need to sum the joint probability distribution over $h=0$ and $h=1$ to find the marginal distribution of the visible units,
\begin{equation}
P(\bs{x})\propto\exp\Big(-\sum_{i=1}^F\frac{(x_i-a_i)^2}{2\sigma^2}\Big)\prod_{j=1}^H\bigg(1+\exp\Big(b_j+\sum_{i=1}^F\frac{w_{ij}x_i}{\sigma^2}\Big)\bigg),
\label{eq:RBMWF2}
\end{equation}
which is shown thoroughly in appendix \ref{app:rbmderive}. Since the visible units take continuous values, we need to integrate to find the marginal distribution of the hidden units, but since we never will use that distribution in this work, we will ignore the marginal distribution of the hidden units.

The conditional distributions are important in Gibbs sampling, and read
\begin{equation}
P(\bs{h}|\bs{x})=\frac{P(\bs{x},\bs{h})}{P(\bs{x})}=\prod_{j=1}^H\frac{\exp(h_jb_j+\sum_{i=1}^Fx_iw_{ij}h_j/\sigma^2)}{1+\exp(b_j+\sum_{i=1}^Fx_iw_{ij}/\sigma^2)}
\end{equation}
and
\begin{equation}
P(\bs{x}|\bs{h})=\mathcal{N}(\bs{x};\bs{a}+\bs{w}^T\bs{h},\sigma^2)
\label{eq:normal}
\end{equation}
where the latter is assumed to be given by a normal distribution in a Bayesian scheme. Note that the mean is $\bs{\mu}=\bs{a}+\bs{w}^T\bs{h}$, which is the vector obtained when going backwards in the restricted Boltzmann machine (multiplying the hidden units with the weights).

In Metropolis sampling, we only use the marginal distribution of the visible units use the weights to the hidden units are additional variational parameters. For completeness reasons, we will discuss the Gibbs sampling, but we will in practice stick to the Metropolis sampling. More about the different sampling tools can be found in chapter \ref{chp:systems}.

We also need the gradient of the log-likelihood function in order to train the network. The likelihood function is defined as the probability is $\bs{x}$ given a set of parameters $\bs{\theta}$, which relate to our problem as $P(\bs{x}|\bs{a},\bs{b},\bs{w})$. We therefore get three maximum log-likelihood estimates,
\begin{align}
\frac{\partial\ln P(\bs{x}|\bs{a},\bs{b},\bs{w})}{\partial\bs{a}}&=\frac{\bs{x}-\bs{a}}{\sigma^2}\\
\frac{\partial\ln P(\bs{x}|\bs{a},\bs{b},\bs{w})}{\partial\bs{b}}&=\bs{n}\\
\frac{\partial\ln P(\bs{x}|\bs{a},\bs{b},\bs{w})}{\partial\bs{w}}&=\frac{\bs{x}\bs{n}^T}{\sigma^2}
\end{align}
which will be used later to maximize the likelihood with respect to the respective set of parameters. 

\subsection{Partly Restricted Boltzmann Machines}
One can also imagine a partly restricted architecture, where we have connections inwards the visible units, but not the hidden units. This is what we have decided to call a partly restricted Boltzmann machine. A such neural network with three visible units and three hidden units is illustrated in figure \eqref{fig:partly_restricted_boltzmann_machine}.

\begin{figure} [H]
	\centering
	\input{tikz/partly_restricted_boltzmann_machine.tex}
	\caption{Partly restricted Boltzmann machine. Black lines are inter-layer connections, where for instance the line between $x_1$ and $h_1$ is related to the weight $w_{11}$. The blue lines are related to the input bias weights, and, for instance, the line going from the bias node to $x_3$ is related to $a_3$. Similarly, the green lines are related to the hidden units bias weights, and, for instance, the line going from the bias node to $h_3$ is related to $b_3$. Finally, the red lines are the intra-layer connections related to the intra-layer weights. The weight between node $x_1$ and $x_2$ is called $c_{12}$. }
	\label{fig:partly_restricted_boltzmann_machine}
\end{figure}

Compared to a standard restricted Boltzmann machine, we get an extra term in the energy expression where the visible units are connected. It is easy to find that the expression should be

\begin{equation}
E(\bs{x},\bs{h})= \sum_{i=1}^F\frac{(x_i-a_i)^2}{2\sigma_i^2} - \sum_{i=1}^F\sum_{j>i}^Fx_ic_{ij}x_j- \sum_{j=1}^Hh_jb_j-\sum_{i=1}^F\sum_{j=i}^H \frac{x_iw_{ij}h_j}{\sigma_i^2} 
\label{eq:partlygaussianbinary}
\end{equation}
with $c_{ij}$ as the weights between the visible units. For the later calculations, we are interested in the marginal distribution only, which reads

\begin{equation}
P(\bs{x})=\exp\Big(-\sum_{i=1}^F\frac{(x_i-a_i)^2}{2\sigma^2}+\sum_{i=1}^F\sum_{j>i}^Fx_ic_{ij}x_j\Big)\prod_{j=1}^H\bigg(1+\exp\Big(b_j+\sum_{i=1}^F\frac{w_{ij}x_i}{\sigma^2}\Big)\bigg).
\label{eq:PRBMWF}
\end{equation}

In chapter \ref{chp:WFE}, we utilize that this marginal distribution can be split in a Gaussian part, a \textit{partly restricted} part and a product part. Then we see that the expression of the gradient of the log-likelihood function becomes the same with respect to $\bs{a},\bs{b}$ and $\bs{w}$ compared to the restricted Boltzmann machine, which means that we only need to calculate the expression of the gradient of the log-likelihood with respect to $\bs{c}$. This is given by the outer product
\begin{equation}
\frac{\partial\ln P(\bs{x}|\bs{a},\bs{b},\bs{c},\bs{w})}{\partial \bs{c}}=\bs{x}\bs{x}^T.
\end{equation}

\subsection{Deep Boltzmann Machines}
We can also construct deep Boltzmann machines, where we just stack single-layer Boltzmann machines. There are many ways to construct those networks, where the number of layers, unit types, number of units and the degree of restriction can be chosen as the constructor wants. The number of combinations is endless, but in order to make use of the dept, all the layer should have different configurations. Otherwise, the deep network can be reduced to a shallower network. In figure \eqref{fig:deep_restricted_boltzmann_machine} a restricted Boltzmann machine of two hidden layers is illustrated. We have chosen three hidden units in each layer, and three visible units. It should be trivial to imagine how the network can be expanded to more layers. 
\begin{figure} [H]
	\centering
	\input{tikz/deep_boltzmann_machine.tex}
	\caption{Deep restricted Boltzmann machine. Black lines the inter-layer connections, where for instance the line between $x_1$ and $h_1$ is related to the weight $w_{11}$. The blue lines are related to the input bias weights, and, for instance, the line going from the bias node to $x_3$ is related to $a_3$. Similarly, the green lines are related to the hidden units bias weights, and, for instance, the line going from the bias node to $h_3$ is related to $b_3$.}
	\label{fig:deep_restricted_boltzmann_machine}
\end{figure}

As the main focus so far has been restricted Boltzmann machines, also the deep networks will be assumed to be restricted, although both partly restricted and unrestricted can be constructed. The system energy of a deep restricted Boltzmann machine of $L$ layers can be expressed as
\begin{equation}
E(\bs{x},\bs{h})= \sum_{i=1}^F\frac{(x_i-a_i)^2}{2\sigma_i^2} - \sum_{l=1}^L\sum_{j=1}^{H_L}h_j^lb_j^l-\sum_{l=1}^L\sum_{i=1}^F\sum_{j=i}^{H_L} \frac{x_iw_{ij}^lh_j^l}{\sigma_i^2}
\label{eq:deepgaussianbinary}
\end{equation}
where $H_L$ is the number of hidden units in layer $L$. The marginal probability distribution of the visible units read
\begin{equation}
P(\bs{x})=\exp\Big(-\sum_{i=1}^F\frac{(x_i-a_i)^2}{2\sigma^2}\Big)\prod_{l=1}^L\prod_{j=1}^{H_L}\bigg(1+\exp\Big(b_j^l+\sum_{i=1}^F\frac{w_{ij}^lx_i}{\sigma^2}\Big)\bigg).
\label{eq:DRBMWF}
\end{equation}
which again can be obtained from the general expressions in appendix \ref{app:rbmderive}.