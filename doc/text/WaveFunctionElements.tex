\chapter{Derivation of Wave Function Elements} \label{chp:WFE}
In chapter \eqref{chp:quantum} we presented the basic principles behind a many-body trial wave function, including the Slater determinant and the well-known Padé-Jastrow factor. Further in chapter \eqref{chp:systems}, the common basis functions of the quantum dot and atomic systems were given, and in the previous chapter, \eqref{chp:machinelearning}, we explained how to create wave functions using Boltzmann machines. This means that all wave function elements used in this thesis already are presented, and in this chapter they are all collected, together with their derivatives and various optimizations. The calculations below are based on two main assumptions:
\begin{enumerate}
	\item For each time step, we change one position coordinate only, i.e, move a particle along one of the principal axis.
	\item A variational parameter $\alpha_i$ appears in only one of the wave function elements.
\end{enumerate}
The first assumption is useful when updating position dependent arrays. Typically, we only need to update a coordinate of an array or a row of a matrix when this assumption is raised, which is hugely beneficial with respect to the computational time. The last assumption is makes all wave function elements independent, which obviously makes life easier. 

The trial wave function $\Psi_T$ is a product of all the $p$ wave function elements $\{\psi_1, \psi_2\hdots\psi_p\}$,
\begin{equation}
\Psi_T(\bs{r}) = \prod_{i=1}^p\psi_i(\bs{r}),
\label{eq:elementproduct}
\end{equation}
and we will now show how this can be used to split up the local energy and the parameter update in element-wise calculations.

\section{Kinetic Energy Calculations}
The local energy, defined in equation \eqref{eq:local energy}, is
\begin{align}
E_L &=\frac{1}{\Psi_T(\bs{r})}\hat{\mathcal{H}}\Psi_T(\bs{r})\\
&=\sum_{k=1}^M\Big[-\frac{1}{2}\Big(\frac{1}{\Psi_T(\bs{r})}\nabla_k^2\Psi_T(\bs{r})\Big) + \mathcal{V}\Big].
\end{align}
The first term, which is the kinetic energy term, is the only wave function-dependent one. It will in this chapter be evaluated for various wave function elements. From the definition of differentiation of a logarithm, we have that
\begin{equation}
\frac{1}{\Psi_T(\bs{r})}\nabla_k\Psi_T(\bs{r})=\nabla_k\ln\Psi_T(\bs{r}),
\end{equation}
which provides the following useful relation 
\begin{equation}
\frac{1}{\Psi_T(\bs{r})}\nabla_k^2\Psi_T(\bs{r})=\nabla_k^2\ln\Psi_T(\bs{r}) + (\nabla_k\ln\Psi_T(\bs{r}))^2.
\end{equation}
Using the fact that the trial wave function is a product of all the elements, the kinetic energy is calculated by
\begin{equation}
\frac{1}{\Psi_T(\bs{r})}\nabla_k^2\Psi_T(\bs{r})=\sum_{i=1}^p\nabla_k^2\ln\psi_i(\bs{r}) + \Big(\sum_{i=1}^p\nabla_k\ln\psi_i(\bs{r})\Big)^2,
\end{equation}
which can be found when all local derivatives $\nabla_k^2\ln\psi_i(\bs{r})$ and $\nabla_k\ln\psi_i(\bs{r})$ are given. For each wave function element given below, those local derivatives will be evaluated. In addition, we need to know the derivative of local energy with respect to the variational parameters in order to update the parameters correctly. 

\section{Parameter update}
In gradient based optimization methods, as we use, one needs to know the gradient of the expectation value of local energy with respect to all variational parameters $\alpha_i$, 
\begin{equation}
\partial_{\alpha_j} \langle E_L\rangle\equiv\frac{\partial \langle E_L(\alpha_j)\rangle}{\partial \alpha_j}.
\end{equation}
This gradient is given by
\begin{equation}
\partial_{\alpha_j} \langle E_L\rangle=2\Big(\langle E_L\partial_{\alpha_j}\ln\Psi_T\rangle - \langle E_L\rangle\langle\partial_{\alpha_j}\ln\Psi_T\rangle\Big)
\end{equation}
which means that we need to calculate the expectation values $\langle E_L\partial_{\alpha_j}\ln\Psi_T\rangle$ and $\langle\partial_{\alpha_j}\ln\Psi_T\rangle$ in addition to the local energy. Those expectation values are found from the integrals
\begin{equation}
\langle\partial_{\alpha_j}\ln\Psi_T\rangle = \int_{-\infty}^{\infty}d\bs{r}P(\bs{r})\partial_{\alpha_j}\ln\Psi_T(\bs{r})
\end{equation}
and
\begin{equation}
\langle E_L\partial_{\alpha_i}\ln\Psi_T\rangle = \int_{-\infty}^{\infty}d\bs{r}P(\bs{r})E_L(\bs{r})\partial_{\alpha_i}\ln\Psi_T(\bs{r}),
\end{equation}
which can also be found by Monte-Carlo integration in the same way as the local energy:
\begin{equation}
\langle\partial_{\alpha_i}\ln\Psi_T\rangle\approx \frac{1}{M}\sum_{i=1}^M\partial_{\alpha_j}\ln\Psi_T(\bs{r}_i)
\end{equation}
and
\begin{equation}
\langle E_L\partial_{\alpha_i}\ln\Psi_T\rangle\approx \frac{1}{M}\sum_{i=1}^ME_L(\bs{r}_i)\partial_{\alpha_j}\ln\Psi_T(\bs{r}_i).
\end{equation}

By again applying equation \eqref{eq:elementproduct}, we have that
\begin{equation}
\partial_{\alpha_j}\ln\Psi_T(\bs{r})=\sum_{i=1}^p\partial_{\alpha_j}\ln\psi_i(\bs{r}).
\end{equation}

\section{Optimizations}
How much a wave function element can be optimized heavily depends on the specific form of the element. For instance, sometimes the previous and present $\nabla_k\ln\phi_i$ are closely related, and only differ with a few calculations, while for some other elements they are not related at all. Those subjective optimizations will therefore be described when presenting each wave function element. 

However, there are still optimizations that apply to all elements and give great speed-up. An example is when calculating the ratio between the previous and present wave functions
for all wave function elements instead of the wave function itself. Firstly, this is usually cheaper to calculate than the wave function itself because we are working in the logarithm space. Secondly, the ratio is actually what we use in the sampling, so it is a natural thing to calculate. The total wave function ratio is just the product of all the wave function element ratios
\begin{equation*}
	\frac{\Psi_T(\bs{r}_{\text{new}})}{\Psi_T(\bs{r}_{\text{old}})}=\prod_{i=1}^p\frac{\psi_i(\bs{r}_{\text{new}})}{\psi_i(\bs{r}_{\text{old}})},
\end{equation*}
and below we will calculate this ratio squared since we are going to use that directly in the sampling. 

\section{Derivatives}
\subsection{Simple Gaussian}
A natural starting point is the Gaussian function, since it appears in standard variational Monte-Carlo computations of quantum dot systems. For $N$ number of particles and $NP$ free dimensions, the function is given by
\begin{equation*}
\psi_{\text{sg}}( \bs{x}; \alpha)=\exp\Big(-\frac{1}{2}\omega\alpha\sum_{j=1}^Nr_j^2\Big)=\exp\Big(-\frac{1}{2}\omega\alpha\sum_{j=1}^{ND}x_j^2\Big)
\end{equation*}
similarly to the function presented in section \eqref{sec:quantumdots}. $\omega$ is the oscillator strength and $\alpha$ is a variational parameter, which for non-interacting atoms is 1. Due to the presence of $r_i^2$, the function can easily be treated both in Cartesian and spherical coordinates, but in this thesis we will focus on the former.

When changing coordinate $i$, the probability ratio can easily be found to be 
\begin{empheq}[box={\mybluebox[5pt]}]{equation}
\frac{|\psi_{\text{sg}}(\bs{x}_{\text{new}})|^2}{|\psi_{\text{sg}}(\bs{x}_{\text{old}})|^2}=\exp\Big(\omega\alpha(x_{i,\text{old}}^2-x_{i,\text{new}}^2)\Big)
\end{empheq}

The gradient with respect to coordinate $x_k$ is
\begin{empheq}[box={\mybluebox[5pt]}]{equation}
\nabla_k\ln\psi_{\text{sg}}=-\omega\alpha x_k
\end{empheq}
and the corresponding Laplacian is
\begin{empheq}[box={\mybluebox[5pt]}]{equation}
\nabla^2\ln\psi_{\text{sg}}=-ND\omega\alpha.
\end{empheq}
Finally, for the parameter update we have that
\begin{empheq}[box={\mybluebox[5pt]}]{equation}
\partial_{\alpha}\ln\psi_{\text{sg}} = -\frac{1}{2}\omega\sum_{j=1}^Mx_j^2.
\end{empheq}

Since this wave function element is quite simple, there is no special optimization available that will cause a noticeable performance improvement.

\subsection{Simple Jastrow factor}
The simple Jastrow factor was introduced in equation \eqref{eq:SimpleJastrow}, and relatively easy to deal with. 
\begin{equation}
\psi_{\text{sj}}(\bs{r};\bs{\beta})=\exp\Big(\sum_{i=1}^N\sum_{j>i}^N\beta_{ij}r_{ij}\Big)
\end{equation}
The probability ratio is given by
\begin{empheq}[box={\mybluebox[5pt]}]{equation}
\frac{|\psi_{\text{sj}}(\bs{r}_{\text{new}})|^2}{|\psi_{\text{sj}}(\bs{r}_{\text{old}})|^2}=\exp\Big(2\sum_{j=1}^N\beta_{ij}(r_{ij}^{\text{new}}-r_{ij}^{\text{old}})\Big)
\end{empheq}
The gradient and Laplacian read
\begin{empheq}[box={\mybluebox[5pt]}]{equation}
\nabla_k\ln\psi_{\text{sj}}=\sum_{j=1}^N\frac{\beta_{kj}}{r_{kj}}(x_k-x_j)
\end{empheq}
and
\begin{empheq}[box={\mybluebox[5pt]}]{equation}
\nabla^2\ln\psi_{\text{sj}}=\sum_{k=1}^{ND}\sum_{j=1}^N\frac{\beta_{kj}}{r_{kj}}\Big(1-\frac{(x_k-x_j)^2}{r_{kj}^2}\Big)
\end{empheq}
respectively. Finally, the parameter update is given by
\begin{empheq}[box={\mybluebox[5pt]}]{equation}
\partial_{\beta_{ml}}\ln\psi_{\text{sj}}=r_{ml}.
\end{empheq}

\subsection{Padé-Jastrow Factor}
The Padé-Jastrow factor is introduced in order to take care of the correlations. It is specified in equation \eqref{eq:PadeJastrow}, 
\begin{equation*}
\psi_{\text{pj}}(\bs{r}; \beta) = \exp\bigg(\sum_{i=1}^N\sum_{j>i}^N\frac{a_{ij}r_{ij}}{1+\beta r_{ij}}\bigg),
\end{equation*}
where $N$ is the number of particles, $r_{ij}$ is the relative distance between particle $i$ and $j$ and $\beta$ is a variational parameter. One challenge is that we operate in Cartesian coordinates, while the expressed Jastrow factor obviously is easier to handle in spherical coordinates. Since we need to differentiate this with respect to all free dimensions, we need to be careful not confuse the particle indices and coordinate indices. Let us define $i$ as the coordinate index and $i'$ as the index on the corresponding particle. With that notation, the gradient and Laplacian read
\begin{equation*}
\nabla_k\ln\psi_{\text{pj}}=\sum_{j'\neq k'=1}^N\frac{\beta_{k'j'}}{(1+\gamma r_{k'j'})^2}\frac{x_k-x_j}{r_{k'j'}}
\end{equation*}
and
\begin{equation*}
\nabla_k^2\ln\psi_{\text{pj}}=\sum_{j'\neq k'=1}^P\frac{\beta_{k'j'}}{(1+\gamma r_{k'j'})^2}\bigg[1-\Big(1+2\frac{\gamma r_{k'j'}}{1+\gamma r_{k'j'}}\Big)\frac{(x_k-x_j)^2}{r_{k'j'}^2}\bigg]\frac{1}{r_{k'j'}}
\end{equation*}
respectively, where $j$ is a coordinate index for the same direction as $k$ ($k=ndi$ with $d$ as the number of dimensions and $n$ as an integer).

\iffalse
The derivative of those again with respect to $\gamma$ are
\begin{equation*}
\partial_{\gamma}\nabla_k\ln\psi_{\text{pj}} = -2 \sum_{j'\neq k'=1}^P\frac{\beta_{k'j'}}{(1+\gamma r_{k'j'})^3}(x_k-x_j)
\end{equation*}
and
\begin{equation*}
\partial_{\gamma}\nabla_k^2\ln\psi_{\text{pj}} = -2 \sum_{j'\neq k'=1}^P\frac{\beta_{k'j'}}{(1+\gamma r_{k'j'})^3}\bigg[1-4\frac{\gamma r_{k'j'}}{1+\gamma r_{k'j'}}\frac{(x_k-x_j)^2}{r_{k'j'}^2}\bigg]
\end{equation*}
\fi
By defining 
\begin{equation*}
f_{ij}=\frac{1}{1+\gamma r_{ij}}\quad g_{ij}=\frac{x_i-x_j}{r_{i'j'}}\quad h_{ij}=\frac{r_{ij}}{1+\gamma r_{ij}}
\end{equation*}
the equations can be written as
\begin{empheq}[box={\mybluebox[5pt]}]{align}
\frac{|\psi_{\text{pj}}(\bs{r}_{\text{new}})|^2}{|\psi_{\text{pj}}(\bs{r}_{\text{old}})|^2}&=\exp\Big(2\sum_{j'=1}^N\beta_{i'j'}(h_{i'j'}^{\text{new}}-h_{i'j'}^{\text{old}})\Big)\notag\\
\nabla_k\ln\psi_{\text{pj}} &=\sum_{j'\neq k'=1}^N\beta_{k'j'}\cdot f_{k'j'}^2\cdot g_{kj}\notag\\
\nabla_k^2\ln\psi_{\text{pj}} &= \sum_{j'\neq k'=1}^N\frac{\beta_{k'j'}}{r_{k'j'}}f_{k'j'}^2\Big[1-(1+2\gamma h_{k'j'})g_{kj}^2\Big]\\
\partial_{\gamma}\nabla_k\ln\psi_{\text{pj}} &=\sum_{j'\neq k'=1}^N\beta_{k'j'}\cdot f_{k'j'}^3(x_k-x_j)\notag\\
\partial_{\gamma}\nabla_k^2\ln\psi_{\text{pj}} &= \sum_{j'\neq k'=1}^N \beta_{k'j'}\cdot f_{k'j'}^3\Big[1=4\gamma h_{k'j'}\cdot g_{kj}^2\Big]\notag,
\end{empheq}
with marked indices ($i'$) as the particle related ones and the unmarked ($i$) as the coordinate related ones. $i'$ is the moved particle. 

\subsection{Slater Determinant}
In general, the the Slater determinant contains all the single particle functions. However, in some cases all single particle functions have the same factor, and then this part can be factorized out. Therefore we will treat the Slater determinant as a normal wave function element in this chapter and in the code. 

As discussed in section \eqref{subsec:slater}, it can be split up in a spin-up part and a spin-down part,
\begin{equation*}
\psi_{\text{sd}}(\bs{x})=
|\hat{D}_{\uparrow}(\bs{x}_{\uparrow})|\cdot |\hat{D}_{\downarrow}(\bs{x}_{\downarrow})|.
\end{equation*}
$x_{\uparrow}$ are the coordinates of particles with spin up (defined as the first half of the coordinates) and $x_{\downarrow}$ are the coordinates of particles with spin down (defined as the last half of the coordinates). 

We can now utilize the logarithmic scale, 
\begin{equation*}
\ln\psi_{\text{sd}}=\ln|\hat{D}_{\uparrow}(\bs{x}_{\uparrow})|+\ln|\hat{D}_{\downarrow}(\bs{x}_{\downarrow})|
\end{equation*}
such that we only need to care about one of the determinants when differentiating, dependent on whether the coordinate we differentiate with respect to is among the spin-up or the spin-down coordinates:
\begin{equation*}
\nabla_k\ln\psi_{\text{sd}}=
\begin{cases} 
\nabla_k\ln|\hat{D}_{\uparrow}(\bs{x}_{\uparrow})| & \text{if} \quad k<N_{\uparrow}\\
\nabla_k\ln|\hat{D}_{\downarrow}(\bs{x}_{\downarrow})| & \text{if} \quad k\geq N_{\uparrow}.
\end{cases}
\end{equation*}
Before we go further, we will introduce a more general notation which cover both cases:
\begin{equation*}
\hat{D}\equiv \hat{D}_{\sigma}(\bs{x}_{\sigma})
\end{equation*}
where $\sigma$ is the spin. When summing, the sum is always over all relevant coordinates. 

Furthermore, we have that
\begin{equation*}
\nabla_k\ln|\hat{D}|=\frac{\nabla_k\detD}{|\hat{D}|}
\end{equation*}
and
\begin{equation*}
\nabla_k^2\ln|\hat{D}|=\frac{\nabla_k^2\detD}{|\hat{D}|}-\bigg(\frac{\nabla_k\detD}{|\hat{D}|}\bigg)^2
\end{equation*}

The first derivative of a determinant is given by Jacobi's formula, which reads
\begin{equation}
\label{eq:jacobi}
\frac{\nabla_i|\hat{A}|}{|\hat{A}|}=\tr(\hat{A}^{-1}\nabla_i\hat{A}),
\end{equation}
and the second derivative is then 
\begin{equation*}
\frac{\nabla_i^2|\hat{A}|}{|\hat{A}|}=\Big(\tr\big(\hat{A}^{-1}\nabla_i\hat{A}\big)\Big)^2+\tr\big(\hat{A}^{-1}\nabla_i^2\hat{A}\big) - \tr\big(\hat{A}^{-1}\nabla_i\hat{A}\hat{A}^{-1}\nabla_i\hat{A}\big)
\end{equation*}
where $\tr\big(\hat{B}\big)$ is the trace of matrix $\hat{B}$, i.e, the sum of all diagonal elements. $\nabla_i\hat{A}$ means that we differentiate the matrix component-wise with respect to coordinate $i$. The traces can then be written as sums,
\begin{equation*}
\tr(\hat{A}^{-1}\nabla_i\hat{A})=\sum_{j}A_{ji}^{-1}\nabla_iA_{ij}.
\end{equation*}
and
\begin{equation*}
	\tr(\hat{A}^{-1}\nabla_i^2\hat{A})=\sum_{j}a_{ji}^{-1}\nabla_i^2a_{ij}.
\end{equation*}

Using all the general matrix operations presented above, we end up with
\begin{equation*}
\nabla_k\ln|\hat{D}|=\sum_{j}d_{jk}^{-1}\nabla_kd_{kj}
\end{equation*}
and
\begin{equation*}
\nabla_k^2\ln|\hat{D}|=\sum_jd_{jk}^{-1}\nabla_k^2d_{kj}-\Big(\sum_jd_{jk}^{-1}\nabla_kd_{kj}\Big)^2
\end{equation*}

\subsubsection{Efficient calculation of Slater determinants}
As you might already have noticed, we need to calculate the inverse of the matrices every time a particle is moved. This is a pretty heavy task for the computer, where the standard way, LU decomposition, requires $\sim$ N$^3$ floating point operations for an N$\times$N matrix. \cite{trahan_computational_2006}. 

The good thing is that, by exploiting that only one row in the Slater matrix is updated for each step, we can update the inverse iteratively. 

Before we start finding an algorithm for this, we will introduce the reader to some common linear algebra concepts. First of all, the inverse of a matrix is given by the \textit{comatrix} transposed over its determinant
\begin{equation}
\hat{A}^{-1}=\frac{\hat{C}^T}{|\hat{A}|}
\end{equation}
where the comatrix is defined by the inner determinants of the matrix. \cite{weisstein_matrix_nodate} As a consequence, the determinant can be written as 
\begin{equation}
|\hat{A}|=\sum_{i,j}a_{ij}c_{ij}.
\end{equation}
As always, we are interested in the ratio between the wave functions, and since only a row is updated every time we move a particle, the ratio between the determinants can be expressed as
\begin{equation}
R\equiv \frac{|\hat{D}^{\text{new}}|}{|\hat{D}^{\text{old}}|}=\frac{\sum_{j}d_{ij}^{\text{new}}c_{ij}^{\text{new}}}{\sum_{j}d_{ij}^{\text{old}}c_{ij}^{\text{old}}}
\end{equation}
where the particle associated with the $i$'th row is moved. The $i$'th row of the comatrix is independent of the $i$'th row of the matrix itself, such that $c_{ij}^{\text{new}}=c_{ij}^{\text{old}}$. 


---------

In the end, we will take advantage of the fact that we only move one particle at a time. This means that one of the two determinants cancel when calculating the probability ratio used in Metropolis sampling. Since we do not have any variational parameters in the Slater determinant, we end up with three expressions for each determinant:

\begin{empheq}[box={\mybluebox[5pt]}]{align}
&\quad\text{if}\quad k<F/2:\notag\\
\frac{|\psi_{\text{sd}}(\bs{x}_{\text{new}})|^2}{|\psi_{\text{sd}}(\bs{x}_{\text{old}})|^2}&=
|\hat{D}_{\uparrow}(\bs{x}_{\uparrow}^{\text{new}})|^2/|\hat{D}_{\uparrow}(\bs{x}_{\uparrow}^{\text{old}})|^2\notag\\
\nabla_k\ln|\hat{D}_{\uparrow}|&=\sum_{j=1}^{M/2}\nabla_kd_{jk}d_{kj}^{-1}\\
\nabla_k^2\ln|\hat{D}_{\uparrow}|&=\sum_{j=1}^{M/2}\nabla_k^2d_{jk}d_{kj}^{-1}-\Big(\sum_{j=1}^{M/2}\nabla_kd_{ik}d_{ki}^{-1}\Big)^2\notag
\end{empheq}

\begin{empheq}[box={\mybluebox[5pt]}]{align}
&\quad\text{if}\quad k\geq F/2:\notag\\
\frac{|\psi_{\text{sd}}(\bs{x}_{\text{new}})|^2}{|\Psi_{\text{sd}}(\bs{x}_{\text{old}})|^2}&=
|\hat{D}_{\downarrow}(\bs{x}_{\downarrow}^{\text{new}})|^2/|\hat{D}_{\downarrow}(\bs{x}_{\downarrow}^{\text{old}})|^2\notag\\
\nabla_k\ln|\hat{D}_{\downarrow}|&=\sum_{j=M/2}^{M}\nabla_kd_{jk}d_{kj}^{-1}\\
\nabla_k^2\ln|\hat{D}_{\downarrow}|&=\sum_{j=M/2}^{M}\nabla_k^2d_{jk}d_{kj}^{-1}-\Big(\sum_{j=M/2}^{M}\nabla_kd_{ik}d_{ki}^{-1}\Big)^2\notag
\end{empheq}

\subsection{Restricted Boltzmann machine}
Now over to the real deal; the wave function elements inspired by machine learning. The total restricted Boltzmann machine (RBM) wave function was presented in equation \eqref{eq:RBMWF2},
\begin{equation}
\psi_{\text{rbm}}(\bs{x};\bs{a},\bs{b},\bs{w})=\exp\Big(-\sum_{i=1}^{ND}\frac{(x_i-a_i)^2}{2\sigma^2}\Big)\prod_{j=1}^H\bigg(1+\exp\Big(b_j+\sum_{i=1}^{ND}\frac{w_{ij}x_i}{\sigma^2}\Big)\bigg).
\end{equation}
and contains a Gaussian part and a product part. In order to minimize the complexity of each wave function element, we decided to split it up in the code and they will therefore be presented separately below. The first part will henceforth be denoted as the RBM-Gaussian, while the last part will be denoted as the RBM-Product. 

\subsubsection{RBM-Gaussian}
The RBM-Gaussian reads
\begin{equation}
\psi_{\text{rg}}(\bs{x};\bs{a})=\exp(-\sum_{i=1}^{ND}\frac{(x_i-a_i)^2}{2\sigma^2})
\end{equation}


The derivatives of the RBM-Gaussian are similar to those of the simple Gaussian, they are therefore just listed in equation \eqref{eq:NQSGaussian}.

\begin{empheq}[box={\mybluebox[5pt]}]{align}
\label{eq:NQSGaussian}
\frac{|\psi_{\text{rg}}(\bs{x}_{\text{new}})|^2}{|\psi_{\text{rg}}(\bs{x}_{\text{old}})|^2}&=\exp\Big((x_i^{\text{old}}+x_i^{\text{new}}-2a_i)(x_i^{\text{old}}-x_i^{\text{new}})\Big)\notag\\
\nabla_k\ln\psi_{\text{rg}} &= -\frac{x_k-a_k}{\sigma^2}\notag\\
\nabla_k^2\ln\psi_{\text{rg}}&=-\frac{1}{\sigma^2}\\
\partial_{\alpha}\ln\psi_{\text{rg}} &= \frac{x_k-a_k}{\sigma^2}\notag
\end{empheq}

\subsubsection{RBM-Product}
The RBM product is the last part of \eqref{eq:RBMWF2}, and is thus given by
\begin{equation*}
\psi_{\text{rp}}(\bs{x};\bs{b},\bs{w})=\prod_{j=1}^H\bigg[1+\exp\Big(b_j+\sum_{i=1}^{ND}\frac{w_{ij}x_i}{\sigma^2}\Big)\bigg].
\end{equation*}
In appendix D, section \eqref{sec:derivatives}, a general Gaussian-binary RBM product on the form
\begin{equation*}
\psi(\bs{x};\bs{\theta})=\prod_{j=1}^H\bigg[1+\exp\Big(f_j(\bs{x};\bs{\theta})\Big)\bigg]
\end{equation*}
is differentiated, which for this element corresponds to setting $f_j=b_j+\bs{w}_j^T\bs{x}/\sigma^2$. As we further claim, the only expressions that need to be calculated are $\nabla_k(f_j)$, $\nabla_k^2(f_j)$ and $\partial_{\theta_i}(f_j)$ for all the coordinates $k$ and all the parameters $\theta_i$. They can easily be found to be 
\begin{align*}
\nabla_k(f_j)&=\frac{w_{kj}}{\sigma^2}\\
\nabla_k^2(f_j)&=0\\
\partial_{b_l}(f_j)&=\delta_{lj}\\
\partial _{w_{ml}}(f_j)&=\frac{x_m}{\sigma^2}\delta_{lj}
\end{align*}
for our specific function. $\delta_{lj}$ is the Kronecker delta. By reintroducing the sigmoid function and the counterpart 
\begin{equation*}
n_j(x)=\frac{1}{1+\exp(-x)}\quad\wedge\quad p_j(x)=n_j(-x)=\frac{1}{1+\exp(x)}
\end{equation*}
we can express the required derivatives in the following fashion
\begin{empheq}[box={\mybluebox[5pt]}]{align}
\frac{|\psi_{\text{rp}}(\bs{x}_{\text{new}})|^2}{|\psi_{\text{rp}}(\bs{x}_{\text{old}})|^2}&=\prod_{j=1}^H\frac{p_j(\bs{x}_{\text{old}})^2}{p_j(\bs{x}_{\text{new}})^2}\notag\\
\nabla_k\ln\psi_{\text{rp}} &=\sum_{j=1}^H\frac{w_{kj}}{\sigma^2}n_j\notag\\
\nabla_k^2\ln\psi_{\text{rp}} &= \sum_{j=1}^H\frac{w_{kj}^2}{\sigma^4}p_jn_j\\
\partial_{b_l}\ln\psi_{\text{rp}}&=n_l\notag\\
\partial_{w_{ml}}\ln\psi_{\text{rp}}&=\frac{x_mn_l}{\sigma^2}.\notag
\end{empheq}
In this element, there are plenty of optimization possibilities. By revealing that some sums are vector products, we can get a significant speed-up. Instead of presenting the vectorized expressions, we will present how the elements actually are implemented.
\lstset{basicstyle=\scriptsize}
\begin{lstlisting}[language=c++]
double prod =  m_pOld.prod() / m_p.prod();
m_probabilityRatio  = prod * prod;

m_gradient = double(m_W.row(k) * m_n) / m_sigmaSqrd;

m_laplacian = (m_w.cwiseAbs2() * m_p.cwiseProd(m_n)).sum() / (m_sigmaSqrd*m_sigmaSqrd);

Eigen::MatrixXd out = m_positions * m_n.transpose();
m_gradients.segment(m_numberOfHiddenNodes, out.size()) = flatten(out);
m_gradients.head(m_numberOfHiddenNodes) = m_n;
\end{lstlisting}
There the linear algebra package Eigen is used for the matrix-vector operations, and \texttt{m\_gradients} consists of all the derivatives with respect to the parameters.

\subsection{Partly restricted Boltzmann machine}
For the partly restricted Boltzmann machine given in equation \eqref{eq:PRBMWF}, we observe that the only difference from a standard Boltzmann machine is the factor 
\begin{equation}
\psi_{\text{pr}}=\exp\Big(\sum_{i=1}^{ND}\sum_{j=1}^{ND}x_ic_{ij}x_j\Big)
\end{equation}
which we can threaten separately. We end up with the expressions
\begin{empheq}[box={\mybluebox[5pt]}]{align}
\frac{|\psi_{\text{pr}}(\bs{x}_{\text{new}})|^2}{|\psi_{\text{pr}}(\bs{x}_{\text{old}})|^2}&=\exp\Big(2\sum_{j=1}^{ND}c_{ij}x_j(x_i^{\text{new}}-x_i^{\text{old}})\Big)\notag\\
\nabla_k\ln\psi_{\text{pr}} &=2\sum_{j=1}^{ND}c_{kj}x_j\notag\\
\nabla_k^2\ln\psi_{\text{pr}} &= 2c_{kk}\\
\partial_{c_{ml}}\ln\psi_{\text{pr}}&=x_mx_l\notag
\end{empheq}
where $x_i$ is the changed coordinated. Also here can we use vectorization to speed-up the computations. 

\subsection{Hydrogen-like orbitals}
\begin{equation}
\psi_{\text{hl}}( \bs{r};\alpha)=\exp\Big(-Z\alpha\sum_{i=j}^Nr_j\Big)
\end{equation}
where the derivative with respect to coordinate $r_k$ is
\begin{equation}
\nabla_k\ln\psi_{\text{hl}}=-Z\alpha\frac{x_k}{r_{k'}}
\end{equation}
and the second derivative is
\begin{equation}
\nabla_k^2\ln\psi_{\text{hl}}=-Z\alpha\Big(1-\frac{x_k^2}{r_{k'}^2}\Big)\frac{1}{r_{k'}}
\end{equation}
The gradient for those derivatives is
\begin{equation}
\partial_{\alpha}\ln\psi_{\text{hl}}=-Z\sum_{j=1}^Nr_j
\end{equation}.