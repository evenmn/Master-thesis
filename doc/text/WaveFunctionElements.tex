\chapter{Derivation of Wave Function Elements} \label{chp:WFE}
In chapter \eqref{chp:quantum} we presented the basic principles behind a many-body trial wave function, including the Slater determinant and the well-known Padé-Jastrow factor. Further, in chapter \eqref{chp:systems}, the common basis functions of the harmonic oscillator and atomic systems were given, and in the previous chapter, \eqref{chp:machinelearning}, we explained how to create wave functions using machine learning. This means that all wave function elements used in this thesis already are given, and in this chapter they are all collected, together with their derivatives and various optimizations. The calculations below are based on two main assumptions:
\begin{itemize}
	\item For each step, we move one particle in one direction.
	\item A variational parameter occurs in only one of the wave function elements.
\end{itemize}
The first assumptions is necessary when we calculate the ratio between the new and the old wave function used in Metropolis sampling, and we need the last to simplify the update of parameters. We start with splitting up the kinetic energy calculations and parameter update to see which derivatives we need.

\section{Kinetic Energy Calculations}
The local energy, defined in equation \eqref{eq:local energy}, is
\begin{align}
E_L &=\frac{1}{\Psi_T}\hatH\Psi_T\\
&=\sum_{k=1}^M\Big[-\frac{1}{2\Psi_T}\nabla_k^2\Psi_T + U_k + V_k\Big].
\end{align}
The first term, which is the kinetic energy term, is the only wave function-dependent one. It will in this chapter be evaluated for various wave function elements. From the definition of differentiation of a logarithm, we have that
\begin{equation}
\frac{1}{\Psi_T}\nabla_k\Psi_T=\nabla_k\ln\Psi_T,
\end{equation}
which provides the following useful relation 
\begin{equation}
\frac{1}{\Psi_T}\nabla_k^2\Psi_T=\nabla_k^2\ln\Psi_T + (\nabla_k\ln\Psi_T)^2.
\end{equation}
Consider a trial wave function, $\Psi_T$, consisting of a product of $p$ wave function elements, $\{\phi_1, \phi_2\hdots\phi_p\}$,
\begin{equation}
\Psi_T = \prod_{i=1}^p\phi_i.
\end{equation}
The kinetic energy related to this trial wave function is then computed by
\begin{equation}
\frac{1}{\Psi_T}\nabla_k^2\Psi_T=\sum_{i=1}^p\nabla_k^2\ln\phi_i + \Big(\sum_{i=1}^p\nabla_k\ln\phi_i\Big)^2,
\end{equation}
which can be found when all local derivatives $\nabla_k^2\ln\phi_i$ and $\nabla_k\ln\phi_i$ are given. For each wave function element given below, those local derivatives will be evaluated. In addition, we need to know the derivative of the local energy with respect to the variational parameters in order to update the parameters correctly. 

\section{Parameter Update}
In gradient based optimization methods, as we use, one needs to know the gradient of the local energy with respect to all variational parameters $\alpha_i$, 
\begin{equation}
\partial_{\alpha_i} \langle E_L\rangle\equiv\frac{\partial \langle E_L(\alpha_i)\rangle}{\partial \alpha_i}.
\end{equation}
Since we are dealing with an expectation value, this gradient can be found from
\begin{equation}
\partial_{\alpha_i} \langle E_L\rangle=2\Big(\langle E_L\partial_{\alpha_i}\ln\Psi_T\rangle - \langle E_L\rangle\langle\partial_{\alpha_i}\ln\Psi_T\rangle\Big)
\end{equation}
which means that we need to calculate the expectation values $\langle E_L\partial_{\alpha_i}\ln\Psi_T\rangle$ and $\langle\partial_{\alpha_i}\ln\Psi_T\rangle$ in addition to the local energy, which is done by Monte-Carlo integration. 

\section{Optimizations}
How much a wave function element can be optimized heavily depends on the specific form of the element. For instance, sometimes the previous and present $\nabla_k\ln\phi_i$ are closely linked, and only differ from each other with a few calculations, while for some elements they are not related at all. Those subjective optimization will therefore be described when presenting each wave function element. 

However, there are still optimizations that apply to all elements and give great speed-up. An example is to calculate the ratio between the previous and present wave functions,
\begin{equation*}
\frac{\Psi_{\text{new}}}{\Psi_{\text{old}}}
\end{equation*}
instead of the wave function itself. Firstly, this is usually cheaper to calculate than the wave function because we are living in the logarithmic space. Secondly, the ratio is actually what we use in the sampling, so it is a natural thing to calculate. 

\section{Derivatives}
\subsection{Simple Gaussian}
A natural starting point is the Gaussian function, since it appears in traditional variational Monte-Carlo computations of the harmonic oscillator. It is also quite simple to evaluate both in Cartesian and spherical coordinates. We are interested in the former. For $M$ free dimensions, the function is given by
\begin{equation*}
\Psi( \bs{x}; \alpha)=\exp\Big(-\frac{1}{2}\omega\alpha\sum_{i=1}^Mx_i^2\Big)
\end{equation*}
where the gradient with respect to coordinate $x_k$ is
\begin{equation*}
\nabla_k\ln\Psi(\alpha)=-\omega\alpha x_k
\end{equation*}
and the corresponding Laplacian is
\begin{equation*}
\nabla_k^2\ln\Psi(\alpha)=-\omega\alpha.
\end{equation*}
Finally, for the parameter update, we have that
\begin{equation}
\partial_{\alpha}\ln\Psi = -\frac{1}{2}\omega\sum_{i=1}^Mx_i^2.
\end{equation}
The most natural thing to 
The only thing that can be optimized regarding this element, is the ratio between the new and the old probability. All expressions are collected below
\begin{empheq}[box={\mybluebox[5pt]}]{align}
\frac{\Psi_{\text{new}}^2}{\Psi_{\text{old}}^2}&=\exp\Big(\alpha\omega(x_{i,\text{old}}^2-x_{i,\text{new}}^2)\Big)\notag\\
\nabla_k\ln\Psi&=-\omega\alpha x_k\notag\\
\nabla_k^2\ln\Psi&=-\omega\alpha\\
\partial_{\alpha}\ln\Psi &= -\frac{1}{2}\omega\sum_{i=1}^Mx_i^2,\notag
\end{empheq}
where $i$ is the changed coordinate. Since the final expressions are all neat, there are no further optimizations available. 

\subsection{Padé-Jastrow Factor}
The Padé-Jastrow factor is introduced in order to take care of the correlations. It is specified in equation \eqref{eq:PadeJastrow}, 
\begin{equation*}
J(\bs{r}; \beta, \gamma) = \exp\bigg(\sum_{i=1}^N\sum_{j=1}^N\frac{a_{ij}r_{ij}}{1+\beta r_{ij}}\bigg),
\end{equation*}
where $N$ is the number of particles. One challenge is that we operate in cartesian coordinates, while the expressed Jastrow factor obviously is easier to handle in spherical coordinates. Since we need to differentiate this with respect to all free dimensions, we need to be careful not confuse the particle indices and coordinate indices. Let us define $i$ as the coordinate index and $i'$ as the index on the corresponding particle. We then get the first and second derivatives
\begin{equation*}
\nabla_k\ln J=\sum_{j'\neq k'=1}^N\frac{\beta_{k'j'}}{(1+\gamma r_{k'j'})^2}\frac{x_k-x_j}{r_{k'j'}}
\end{equation*}
and
\begin{equation*}
\nabla_k^2\ln J=\sum_{j'\neq k'=1}^N\frac{\beta_{k'j'}}{(1+\gamma r_{k'j'})^2}\bigg[1-\Big(1+2\frac{\gamma r_{k'j'}}{1+\gamma r_{k'j'}}\Big)\frac{(x_k-x_j)^2}{r_{k'j'}^2}\bigg]\frac{1}{r_{k'j'}}
\end{equation*}
respectively, where $j$ yields the same dimension as $k$.

The derivative of those again with respect to $\gamma$ are
\begin{equation*}
\partial_{\gamma}\nabla_k\ln J = -2 \sum_{j'\neq k'=1}^N\frac{\beta_{k'j'}}{(1+\gamma r_{k'j'})^3}(x_k-x_j)
\end{equation*}
and
\begin{equation*}
\partial_{\gamma}\nabla_k^2\ln J = -2 \sum_{j'\neq k'=1}^N\frac{\beta_{k'j'}}{(1+\gamma r_{k'j'})^3}\bigg[1-4\frac{\gamma r_{k'j'}}{1+\gamma r_{k'j'}}\frac{(x_k-x_j)^2}{r_{k'j'}^2}\bigg]
\end{equation*}
By defining 
\begin{equation*}
f_{ij}=\frac{1}{1+\gamma r_{ij}}\quad g_{ij}=\frac{x_i-x_j}{r_{i'j'}}\quad h_{ij}=\frac{r_{ij}}{1+\gamma r_{ij}}
\end{equation*}
the equations can be written as
\begin{empheq}[box={\mybluebox[5pt]}]{align}
\frac{J_{\text{new}}^2}{J_{\text{old}}^2}&=\exp\Big(2\sum_{j'=1}^N\beta_{i'j'}(h_{i'j'}^{\text{new}}-h_{i'j'}^{\text{old}})\Big)\notag\\
\nabla_k\ln J &=\sum_{j'\neq k'=1}^N\beta_{k'j'}\cdot f_{k'j'}^2\cdot g_{kj}\notag\\
\nabla_k^2\ln J &= \sum_{j'\neq k'=1}^N\frac{\beta_{k'j'}}{r_{k'j'}}f_{k'j'}^2\Big[1-(1+2\gamma h_{k'j'})g_{kj}^2\Big]\\
\partial_{\gamma}\nabla_k\ln J &=\sum_{j'\neq k'=1}^N\beta_{k'j'}\cdot f_{k'j'}^3(x_k-x_j)\notag\\
\partial_{\gamma}\nabla_k^2\ln J &= \sum_{j'\neq k'=1}^N \beta_{k'j'}\cdot f_{k'j'}^3\Big[1=4\gamma h_{k'j'}\cdot g_{kj}^2\Big]\notag,
\end{empheq}
with marked indices ($i'$) as the particle related ones and the unmarked ($i$) as the coordinate related ones. $i'$ is the moved particle. 

\subsection{Slater Determinant}
As described in section \eqref{subsec:slater}, the Slater determinant can be split up in a spin-up part and a spin-down part,
\begin{equation*}
\Psi(\bs{x})=
|\hat{D}_{\uparrow}(\bs{x}_{\uparrow})|\cdot |\hat{D}_{\downarrow}(\bs{x}_{\downarrow})|
\end{equation*}
where $x_{\uparrow}$ are the coordinates of particles with spin up (defined as the first half of the coordinates) and $x_{\downarrow}$ are the coordinates of particles with spin down (defined as the last half of the coordinates). 

We can now utilize the logarithmic scale, 
\begin{equation*}
\ln\Psi=\ln|\hat{D}_{\uparrow}(\bs{x}_{\uparrow})|+\ln|\hat{D}_{\downarrow}(\bs{x}_{\downarrow})|
\end{equation*}
such that we only need to care one of the determinants when differentiating, dependent on whether the coordinate we differentiate with respect to is among the spin-up or the spin-down coordinates:
\begin{equation*}
\nabla_k\ln\Psi=
\begin{cases} 
\nabla_k\ln|\hat{D}_{\uparrow}(\bs{x}_{\uparrow})| & \text{if} \quad k<M/2\\
\nabla_k\ln|\hat{D}_{\downarrow}(\bs{x}_{\downarrow})| & \text{if} \quad k\geq M/2.
\end{cases}
\end{equation*}
Before we go further, we will therefore introduce a more general notation which cover both cases:
\begin{equation*}
\hat{D}\equiv \hat{D}_{\sigma}(\bs{x}_{\sigma})
\end{equation*}
where $\sigma$ is the spin. When summing, the sum is always over all relevant coordinates. 

Furthermore, we have that
\begin{equation*}
\nabla_k\ln|\hat{D}|=\frac{\nabla_k\detD}{|\hat{D}|}
\end{equation*}
and
\begin{equation*}
\nabla_k^2\ln|\hat{D}|=\frac{\nabla_k^2\detD}{|\hat{D}|}-\bigg(\frac{\nabla_k\detD}{|\hat{D}|}\bigg)^2
\end{equation*}

The first derivative of a determinant is given by Jacobi's formula, which reads
\begin{equation}
\label{eq:jacobi}
\frac{\nabla_i|\hat{A}|}{|\hat{A}|}=\tr(\hat{A}^{-1}\nabla_i\hat{A}),
\end{equation}
and the second derivative is then 
\begin{equation*}
\frac{\nabla_i^2|\hat{A}|}{|\hat{A}|}=\bigg[\Big(\tr\big(\hat{A}^{-1}\nabla_i\hat{A}\big)\Big)^2+\tr\big(\hat{A}^{-1}\nabla_i^2\hat{A}\big) - \tr\big(\hat{A}^{-1}\nabla_i\hat{A}\hat{A}^{-1}\nabla_i\hat{A}\big)\bigg]
\end{equation*}
where $\tr\big(\hat{B}\big)$ is the trace of matrix $\hat{B}$, i.e, the sum of all diagonal elements. 

We continue writing the trace as a sum, such that
\begin{equation*}
\tr(\hat{A}^{-1}\nabla_i\hat{A})=\sum_{j=1}^NA_{ji}^{-1}\nabla_iA_{ij}
\end{equation*}
and
\begin{equation*}
\tr(\hat{A}^{-1}\nabla_i^2\hat{A})=\sum_{j=1}^NA_{ji}^{-1}\nabla_i^2A_{ij}.
\end{equation*}

In the end, we will take advantage of the fact that we only move one particle at a time. This means one of the who determinants cancel when calculating the probability ratio used in Metropolis sampling. Since we do not have any variational parameters in the Slater determinant, we end up with three expressions for each determinant:

\begin{empheq}[box={\mybluebox[5pt]}]{align}
&\quad\text{if}\quad k<M/2:\notag\\
\frac{|\Psi_{\text{new}}|^2}{|\Psi_{\text{old}}|^2}&=
|\hat{D}_{\uparrow}(\bs{x}_{\uparrow}^{\text{new}})|^2/|\hat{D}_{\uparrow}(\bs{x}_{\uparrow}^{\text{old}})|^2\notag\\
\nabla_k\ln|\hat{D}_{\uparrow}|&=\sum_{j=1}^{M/2}\nabla_kd_{jk}d_{kj}^{-1}\\
\nabla_k^2\ln|\hat{D}_{\uparrow}|&=\sum_{j=1}^{M/2}\nabla_k^2d_{jk}d_{kj}^{-1}-\Big(\sum_{j=1}^{M/2}\nabla_kd_{ik}d_{ki}^{-1}\Big)^2\notag
\end{empheq}

\begin{empheq}[box={\mybluebox[5pt]}]{align}
&\quad\text{if}\quad k\geq M/2:\notag\\
\frac{|\Psi_{\text{new}}|^2}{|\Psi_{\text{old}}|^2}&=
|\hat{D}_{\downarrow}(\bs{x}_{\downarrow}^{\text{new}})|^2/|\hat{D}_{\downarrow}(\bs{x}_{\downarrow}^{\text{old}})|^2\notag\\
\nabla_k\ln|\hat{D}_{\downarrow}|&=\sum_{j=M/2}^{M}\nabla_kd_{jk}d_{kj}^{-1}\\
\nabla_k^2\ln|\hat{D}_{\downarrow}|&=\sum_{j=M/2}^{M}\nabla_k^2d_{jk}d_{kj}^{-1}-\Big(\sum_{j=M/2}^{M}\nabla_kd_{ik}d_{ki}^{-1}\Big)^2\notag
\end{empheq}

\subsubsection{Efficient calculation of Slater determinants}
As you might already have noticed, we need to calculate the inverse of the matrices every time a particle is moved. This is a pretty heavy task for the computer, where the standard way, LU decomposition, gives $\sim N^3$ floating point operations (FLOPS). \cite{trahan_computational_2006}. 

The good thing is that, by exploiting that only one row in the Slater matrix is updated for each step, we can update the inverse iteratively. 

\subsection{NQS-Gaussian}
Now over to the real deal; the machine learning inspired wave function elements. The total NQS wave function, presented in equation \eqref{eq:NQSWF}, was decided split up in case we wanted to run them separately. The first part will henceforth be denoted as the NQS-Gaussian,
\begin{equation}
\Psi(\bs{x};\bs{a})=\exp(-\sum_{i=1}^M\frac{(x_i-a_i)^2}{2\sigma^2})
\end{equation}
while the last part will be denoted as the NQS-Jastrow and is presented in the next subsection. 

The derivatives of the NQS-Gaussian are similar to those of the simple Gaussian, they are therefore just listed up in equation \eqref{eq:NQSGaussian}.

\begin{empheq}[box={\mybluebox[5pt]}]{align}
\label{eq:NQSGaussian}
\frac{\Psi_{\text{new}}^2}{\Psi_{\text{old}}^2}&=\exp\Big((x_i^{\text{old}}+x_i^{\text{new}}-2a_i)(x_i^{\text{old}}-x_i^{\text{new}})\Big)\notag\\
\nabla_k\ln\Psi &= -\frac{x_k-a_k}{\sigma^2}\notag\\
\nabla_k^2\ln\Psi&=-\frac{1}{\sigma^2}\\
\partial_{a_l}\nabla_k\ln\Psi&=\frac{1}{\sigma^2}\notag\\
\partial_{a_l}\nabla_k^2\ln\Psi&=0\notag
\end{empheq}

\subsection{NQS-Jastrow Factor}
\begin{equation*}
J(\bs{x};\bs{b},\bs{W})=\prod_{j=1}^N\bigg[1+\exp\Big(b_j+\sum_{i=1}^M\frac{W_{ij}x_i}{\sigma^2}\Big)\bigg]
\end{equation*}

\begin{equation*}
\nabla_k \ln J=\sum_{j=1}^N\frac{W_{kj}}{\sigma^2}\frac{\exp\big(b_j+\sum_{i=1}^M\frac{W_{ij}x_i}{\sigma^2}\big)}{1+\exp\big(b_j+\sum_{i=1}^M\frac{W_{ij}x_i}{\sigma^2}\big)}
\end{equation*}

\begin{equation*}
\nabla_k^2 \ln J=\sum_{j=1}^N\frac{W_{kj}^2}{\sigma^4}\frac{\exp\big(b_j+\sum_{i=1}^M\frac{W_{ij}x_i}{\sigma^2}\big)}{\Big(1+\exp\big(b_j+\sum_{i=1}^M\frac{W_{ij}x_i}{\sigma^2}\big)\Big)^2}
\end{equation*}

\begin{equation*}
\partial_{b_l}\nabla_k \ln J=\frac{W_{kl}}{\sigma^2}\frac{\exp\big(b_l+\sum_{i=1}^M\frac{W_{il}x_i}{\sigma^2}\big)}{\Big(1+\exp\big(b_l+\sum_{i=1}^M\frac{W_{il}x_i}{\sigma^2}\big)\Big)^2}
\end{equation*}

\begin{equation*}
\partial_{b_l}\nabla_k^2 \ln J=\frac{W_{kl}^2}{\sigma^4}\frac{\exp\big(b_l+\sum_{i=1}^M\frac{W_{il}x_i}{\sigma^2}\big)\Big(1-\exp\big(b_l+\sum_{i=1}^M\frac{W_{il}x_i}{\sigma^2}\big)\Big)}{\Big(1+\exp\big(b_l+\sum_{i=1}^M\frac{W_{il}x_i}{\sigma^2}\big)\Big)^3}
\end{equation*}

\begin{equation*}
\partial_{W_{ml}}\nabla_k \ln J=\frac{1}{\sigma^2}\frac{\exp\big(b_l+\sum_{i=1}^M\frac{W_{il}x_i}{\sigma^2}\big)}{1+\exp\big(b_l+\sum_{i=1}^M\frac{W_{il}x_i}{\sigma^2}\big)}\delta_{mk}
+\frac{W_{kl}x_m}{\sigma^4}\frac{\exp\big(b_l+\sum_{i=1}^M\frac{W_{il}x_i}{\sigma^2}\big)}{\Big(1+\exp\big(b_l+\sum_{i=1}^M\frac{W_{il}x_i}{\sigma^2}\big)\Big)^2}
\end{equation*}

\begin{equation*}
\partial_{W_{ml}}\nabla_k^2 \ln J=2\frac{W_{kl}}{\sigma^4}\frac{\exp\big(b_l+\sum_{i=1}^M\frac{W_{il}x_i}{\sigma^2}\big)}{\Big(1+\exp\big(b_l+\sum_{i=1}^M\frac{W_{il}x_i}{\sigma^2}\big)\Big)^2}\delta_{mk}
+\frac{W_{kl}^2x_m}{\sigma^4}\frac{\exp\big(b_l+\sum_{i=1}^M\frac{W_{il}x_i}{\sigma^2}\big)}{\Big(1+\exp\big(b_l+\sum_{i=1}^M\frac{W_{il}x_i}{\sigma^2}\big)\Big)^3}
\end{equation*}
where $\delta_{ij}$ is the Kronecker delta. Defining 
\begin{equation*}
p_j\equiv \frac{1}{1+\exp\big(+b_j+\sum_{i=1}^M\frac{W_{ij}x_i}{\sigma^2}\big)}\quad\text{and}\quad n_j\equiv \frac{1}{1+\exp\big(-b_j-\sum_{i=1}^M\frac{W_{ij}x_i}{\sigma^2}\big)}
\end{equation*}
the expressions above can be simplified in the following fashion
\begin{empheq}[box={\mybluebox[5pt]}]{align}
\frac{J_{\text{new}}^2}{J_{\text{old}}^2}&=\prod_{j=1}^N\frac{p_j^{\text{old}}}{p_j^{\text{new}}}\notag\\
\nabla_k\ln J &=\sum_{j=1}^N\frac{W_{kj}}{\sigma^2}n_j\notag\\
\nabla_k^2\ln J &= \sum_{j=1}^N\frac{W_{kj}^2}{\sigma^4}p_jn_j\notag\\
\partial_{b_l}\nabla_k\ln J &=\frac{W_{kl}}{\sigma^2}p_ln_l\\
\partial_{b_l}\nabla_k^2\ln J &=\frac{W_{kl}^2}{\sigma^4}p_ln_l(p_l-n_l)\notag\\
\partial_{W_{ml}}\nabla_k\ln J &=\frac{1}{\sigma^2}n_l\delta_{mk}+\frac{W_{kl}x_m}{\sigma^4}p_ln_l\notag\\
\partial_{W_{ml}}\nabla_k^2\ln J &=2\frac{W_{kl}}{\sigma^4}p_ln_l\delta_{mk}+\frac{W_{kl}^2x_m}{\sigma^6}p_ln_l(p_l-n_l)\notag
\end{empheq}

\subsection{Hydrogen-Like Orbitals}
\begin{equation}
\Psi(\alpha, \bs{r})=\exp\Big[-\frac{1}{2}\alpha\sum_{i=1}^Nr_i\Big]
\end{equation}
where the derivative with respect to coordinate $r_k$ is
\begin{equation}
\nabla_k\ln\Psi(\alpha)=-\alpha
\end{equation}
and the second derivative is
\begin{equation}
\nabla_k^2\ln\Psi(\alpha)=0
\end{equation}
The gradients for those derivatives are
\begin{equation}
\partial_{\alpha} \nabla_k\ln\Psi(\alpha)=-1
\end{equation}
and
\begin{equation}
\partial_{\alpha} \nabla_k^2\ln\Psi(\alpha)=0
\end{equation}
respectively.  