\chapter{Optimization and Statistics} \label{chp:optimization}
\epigraph{Great quote.}{Author}
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.4]{Images/example.png}
	\caption{Caption}
\end{figure}

Optimization is a wide term..

\newpage
\section{Optimization algorithms}
In chapter \ref{chp:machinelearning}, we discussed the gradient descent optimization algorithm, which is among the most basic methods available. That method is based on the gradient, which is the slope of the cost function, but many methods are also in need of the Hessian matrix, which gives the curvature of the cost function. We will barely scratch the surface of this field, limiting us to the gradient methods. 

To have the method fresh in mind, we will start with reintroducing the gradient descent method before we move on the its stochastic brother. We will then have a look at how momentum can be added, and finally we examine the stochastic and momentum based ADAM optimizer. 

\subsection{Gradient descent} \label{sec:gd}
Perhaps the simplest and most intuitive method for finding the minimum is the gradient descent method (GD), which reads
\begin{equation}
\label{eq:GD}
\alpha_i^{\text{new}}=\alpha_i - \eta\cdot\frac{\partial Q(\alpha_i)}{\partial\alpha_i}
\end{equation}
where $\alpha_i^{\text{new}}$ is the updated weight $\alpha$ and $\eta$ is the learning rate. The idea is to find the steepest slope of the cost function $Q(\vec{\alpha})$ with respect to a certain $\alpha_i$, and move in the direction which minimizes the cost function. For every step, the cost function is thus minimized, and when the gradient approaches zero the minimum is found. A possible stop criterion is
\begin{equation}
\frac{\partial Q(\alpha_i)}{\partial\alpha_i}<\varepsilon.
\end{equation}
where $\varepsilon$ is a tolerance. 

In cases where the cost function is not strictly increasing or decreasing, we will have both local and global minima. Often, it is hard to say whether we are stuck in a local or global minimum, and this is where the stochasticity enters the game.

\subsection{Stochastic gradient descent}
For standard gradient descent, we calculate the gradient based on all sampling points, we say that we have one batch. 

Instead of calculating the gradient based on all sampling points, we can divide the data into multiple batches and calculate the gradient based on one batch and hope that it is a good approximation of the true gradient. Updating weight $\alpha_i$ based on batch $j$ thus yields
\begin{equation}
\label{eq:SGD}
\alpha_i^{\text{new}}=\alpha_i - \eta\cdot\frac{\partial Q_j(\alpha_i)}{\partial\alpha_i}
\end{equation}
and a run through all batches is called an \textit{epoch}. 

The reader might ask herself why this helps us, will we not just get a bad gradient approximation? The answer is that the stochasticity adds some coincidence to the system, which makes it less likely to be stuck in local minima. Additionally, it might speed-up the training session. 

\subsection{ADAM}
ADAM is a first-order stochastic optimization method which is widely used in machine learning. It was discovered by D.P. Kingma and J. Ba, and published in a 2014 paper. The article has already more than 20000 citations! \cite{kingma_adam:_2014} So what makes this method so great? 

The main reason why it is so popular might be that it is straight-forward to implement, in the same time as it is efficient and capable of handle a large number of parameters. Additionally, the method has built-in momentum which makes it less likely to be stuck in a local minimum, more about that later.

The algorithm goes as following

\begin{algorithm}[H]
\SetAlgoLined
\KwData{this text}
\KwResult{how to write algorithm with \LaTeX2e }
initialization\;
\While{not at end of this document}{
	read current\;
	\eIf{understand}{
		go to next section\;
		current section becomes this one\;
	}{
		go back to the beginning of current section\;
	}
}
\caption{The ADAM algorithm}
\end{algorithm}

\subsection{Adding momentum}
We have already mentioned momentum, but what is it and why do we use it?

If we go back to an introductory mechanics course, you might remember that momentum is a quantity that maintains the motion of a body. Imagine a ball that rolls down a steep hill, but then there is a local minimum that it needs to escape to keep rolling. Because of its momentum, it will probably be able to escape. 

Exactly the same idea lies behind the momentum used in optimization algorithms; the momentum will try to maintain the motion towards the global minimum, which makes the system less likely to be stuck in a local minimum. Se figure .. for illustration. 

Momentum can be added to most optimization algorithms, also gradient descent and stochastic gradient descent. The way we do it is to ...
\begin{equation}
\bs{v} = \gamma\bs{v} + \eta\nabla E
\end{equation}

\section{Variance estimation}
\begin{equation}
\sigma^2=\langle E_L^2\rangle - \langle E_L\rangle^2
\end{equation}

\section{Resampling analysis}
\subsection{Blocking}
dkdkdk



\section{Random number generators} \label{sec:RNG}
In the Monte-Carlo sampling we are drawing millions of numbers, and in order to get accurate estimations, they should all be random, independent and fast to get. 

In C++ there are plenty of random number generator available, but not all of them meet our requirements. For instance the standard 

Mersenne Twister needs to be mentioned here