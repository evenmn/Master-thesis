\chapter{Optimization and Statistics} \label{chp:optimization}
\epigraph{Great quote.}{Author}
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.4]{Images/example.png}
	\caption{Caption}
\end{figure}

Optimization is a wide term..

\newpage
\section{Minimization Algorithms}
Suppose we have a very simple model trying to fit a straight line to data points. In that case, we could manually vary the coefficients and find a line that fits the points quite good. However, when the model gets more complicated, this can be a time consuming activity. Would it not be good if the program could do this for us?

In fact, there exist plenty of methods capable of doing this. Some of them rely on the gradients only, and are therefore called gradient methods. Other need both the gradient and the Hessian matrix, and find the minimum based on both the slope and the curvature of the cost function. For our purpose, gradients methods have provided good results over decades, and there is no need for more complicated algorithms. The standard gradient descent method will be discussed firstly, before we move to its stochastic brother. Momentum will be added for both methods. Finally, we examine the ADAM optimizer, which is stochastic by nature and is equipped with momentum by default. 

\subsection{Gradient Descent} \label{sec:gd}
Perhaps the simplest and most intuitive method for finding the minimum is the gradient descent method (GD), which reads
\begin{equation}
\label{eq:GD}
\alpha_i^{\text{new}}=\alpha_i - \eta\cdot\frac{\partial Q(\alpha_i)}{\partial\alpha_i}
\end{equation}
where $\alpha_i^{\text{new}}$ is the updated weight $\alpha$ and $\eta$ is the learning rate. The idea is to find the steepest slope of the cost function $Q(\vec{\alpha})$ with respect to a certain $\alpha_i$, and move in the direction which minimizes the cost function. For every step, the cost function is thus minimized, and when the gradient approaches zero the minimum is found. A possible stop criterion is
\begin{equation}
\frac{\partial Q(\alpha_i)}{\partial\alpha_i}<\varepsilon.
\end{equation}
where $\varepsilon$ is a tolerance. 

In cases where the cost function is not strictly increasing or decreasing, we will have both local and global minima. Often, it is hard to say whether we are stuck in a local or global minimum, and this is where the stochasticity enters the game.

\subsection{Stochastic Gradient Descent}
For standard gradient descent, we calculate the gradient based on all sampling points, we say that we have one batch. 

Instead of calculating the gradient based on all sampling points, we can divide the data into multiple batches and calculate the gradient based on one batch and hope that it is a good approximation of the true gradient. Updating weight $\alpha_i$ based on batch $j$ thus yields
\begin{equation}
\label{eq:SGD}
\alpha_i^{\text{new}}=\alpha_i - \eta\cdot\frac{\partial Q_j(\alpha_i)}{\partial\alpha_i}
\end{equation}
and a run through all batches is called an \textit{epoch}. 

The reader might ask herself why this helps us, will we not just get a bad gradient approximation? The answer is that the stochasticity adds some coincidence to the system, which makes it less likely to be stuck in local minima. Additionally, it might speed-up the training session. 

\subsection{ADAM}
ADAM is a first-order stochastic optimization method which is widely used in machine learning. It was discovered by D.P. Kingma and J. Ba, and published in a 2014 paper. The article has already more than 20000 citations! \cite{kingma_adam:_2014} So what makes this method so great? 

The main reason why it is so popular might be that it is straight-forward to implement, in the same time as it is efficient and capable of handle a large number of parameters. Additionally, the method has built-in momentum which makes it less likely to be stuck in a local minimum, more about that later.

The algorithm goes as following

\begin{algorithm}[H]
\SetAlgoLined
\KwData{this text}
\KwResult{how to write algorithm with \LaTeX2e }
initialization\;
\While{not at end of this document}{
	read current\;
	\eIf{understand}{
		go to next section\;
		current section becomes this one\;
	}{
		go back to the beginning of current section\;
	}
}
\caption{The ADAM algorithm}
\end{algorithm}

\subsection{Adding momentum}
We have already mentioned momentum, but what is it and why do we use it?

If we go back to an introductory mechanics course, you might remember that momentum is a quantity that maintains the motion of a body. Imagine a ball that rolls down a steep hill, but then there is a local minimum that it needs to escape to keep rolling. Because of its momentum, it will probably be able to escape. 

Exactly the same idea lies behind the momentum used in optimization algorithms; the momentum will try to maintain the motion towards the global minimum, which makes the system less likely to be stuck in a local minimum. Se figure .. for illustration. 

Momentum can be added to most optimization algorithms, also gradient descent and stochastic gradient descent. The way we do it is to ...
\begin{equation}
\bs{v} = \gamma\bs{v} + \eta\nabla E
\end{equation}

\section{Variance Estimation}
\begin{equation}
\sigma^2=\langle E_L^2\rangle - \langle E_L\rangle^2
\end{equation}

\section{Resampling}
\subsection{Blocking}
dkdkdk



\section{Random number generators} \label{sec:RNG}
In the Monte-Carlo sampling we are drawing millions of numbers, and in order to get accurate estimations, they should all be random, independent and fast to get. 

In C++ there are plenty of random number generator available, but not all of them meet our requirements. For instance the standard 

Mersenne Twister needs to be mentioned here