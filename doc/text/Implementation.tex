\chapter{Implementation} \label{sec:implementation}
\epigraph{There are only two hard things in Computer Science: cache invalidation and naming things.}{Phil Karlton, \cite{fowler_bliki:_nodate}}
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.4]{Images/example.png}
	\caption{Caption}
\end{figure}

In this chapter de will describe the implemented VMC code, which was developed from scratch in C++. As the code itself is around 7000 significant\footnote{Significant lines of code in this sense means lines that are not blank or commented. Counted by the cloc program \cite{aldanial_cloc_2019}.} lines of code, we will just go through selected and often not obvious parts. As often said, \textit{good planning is half the battle}, which largely relates to writing VMC code. The code was rewritten and restructured several times before we ended on the final version. As a starting point, we used Morten Ledum's VMC framework \cite{ledum_simple_2016}, which was meant as an example implementation in the course \textit{FYS4411 - Computational Physics II: Quantum Mechanical Systems}. The entire source code can be found on the authors github, \cite{nordhagen_general_2019}.

For all matrix operations, the open source template library for linear algebra Eigen was used throughout the code. Eigen provides an elegant interface, with support for all the needed matrix and vector operations. In addition, Eigen is built on the standard software libraries for numerical linear algebra, BLAS and LAPACK, which are incredibly fast. These contribute greatly to the performance of the code. 

The code was developed in regards to three principal aims:
\begin{itemize}
	\itemsep-0.3em
	\item flexible,
	\item fast,
	\item readable.
\end{itemize}
It needs to be flexible in order to support the Boltzmann machines as our trial wave function guess, and since we will try out various Jastrow factors it should be easy to add and remove wave function elements. Since quantum mechanical simulations in general are very expensive, it is important to develop efficient code to be able to study systems of some size. Lastly, we aim to write readable code such that others can reuse and rewrite the code later. 

How we work to achieve the goals will be illustrated by code mainly picked from the \texttt{WaveFunction} class, which is the heart of the code. 

\section{Flexibility}
Unlike many other VMC codes, our code was developed flexible with respect to the wave functions. This means that one can combine various wave function elements, where each element is implemented separately. For instance, the Slater determinant with the Gaussian part factorized out and the Padé-Jastrow factor were implemented as three separate elements, but they all can easily be combined. The way one does this in practice, is to append multiple wave function elements to a vector \texttt{WaveFunctionElements}. A quantum dot trial wave function combining the Gaussian with the Padé-Jastrow factor and the Slater determinant can be called in following way

\lstset{basicstyle=\scriptsize}
\begin{lstlisting}[language=c++,caption={Due to the flexibility of the code, wave function elements can be combined in every possible way.}]
System* quantumDot = new System();
std::vector<class WaveFunction*> WaveFunctionElements;
WaveFunctionElements.push_back(new Gaussian(quantumDot));
WaveFunctionElements.push_back(new PadeJastrow(quantumDot));
WaveFunctionElements.push_back(new SlaterDeterminant(quantumDot));
quantumDot->setWaveFunctionElements(WaveFunctionElements);
\end{lstlisting}

The big advantage of this implementation technique is that we do not need to hard code every possible combination of wave function elements, which reduces the number of code lines significantly. This also eases the operation of adding new elements, since we only need to calculate the derivatives of the particular element (do not need to worry about cross terms). The exact derivation of all the wave function elements, with all the required derivatives, can be read in chaper \ref{chp:WFE}. The con is that the program becomes slightly slower, since even canceling cross terms are calculated.

\section{Efficiency}
The efficiency is mostly based on recursive computations, such that we do not need to calculate everything over again when it is enough to calculate the fraction between the old the new number. This is highly relevant when it comes to the Slater determinant, but we also do this for other wave function elements. Those optimizations are described for all the respective elements in chapter \ref{chp:WFE}.

First of all, we need to initialize all position dependent arrays. Some of these are initialized once, and then updated for the rest of the run based on their earlier value. \texttt{initializeArrays} does all the magic here.

Every wave function element is equipped with a function, \texttt{updateArrays}, where all relevant arrays are updated immediately after a particle is moved. In this way we ensure that nothing is calculated twice inside any element. Those functions are by far the most expensive to calculate, but it is also easier to streamline a really expensive function than several quite expensive ones. 

Inside \texttt{updateArrays}, we first need to update the old variables, typically named like \texttt{m\_positionsOld} or so. This is generally done by calling the function \texttt{setArrays}, which ensures that all the old variables are correct. We need to store the old variables in case a move is rejected and we need to go back to the old positions. Thereafter, we can update all the variables. The most basic example is the Gaussian function, where we basically only need to update the position and the probability ratio.

\lstset{basicstyle=\scriptsize}
\begin{lstlisting}[language=c++,caption={from \texttt{gaussian.cpp}}]
void Gaussian::updateArrays(Eigen::VectorXd positions, int changedCoord) {
    setArrays();
    m_positions             = positions;
    updateProbabilityRatio(changedCoord);
}

void Gaussian::setArrays() {
    m_positionsOld          = m_positions;
    m_probabilityRatioOld   = m_probabilityRatio;
}

void Gaussian::updateProbabilityRatio(int changedCoord) {
    m_probabilityRatio = exp(m_omega * m_alpha * (m_positionsOld(changedCoord) * \
    m_positionsOld(changedCoord)- m_positions(changedCoord)*m_positions(changedCoord)));
}
\end{lstlisting}

Similarly to the function \texttt{setArrays}, there is also a function \texttt{resetArrays}, which is called then a move is rejected. It works the exact opposite way, looking like

\lstset{basicstyle=\scriptsize}
\begin{lstlisting}[language=c++,caption={from \texttt{gaussian.cpp}}]
void Gaussian::resetArrays() {
    m_positions             = m_positionsOld;
    m_probabilityRatio      = m_probabilityRatioOld;
}
\end{lstlisting}

There are also a few arrays that are used inside multiple wave function elements, such as the distance matrix and the radial distance vector. They might also be used in the Hamiltonian. To ensure that they are not calculated more than necessary, we define them globally together with a respective Boolean which tells us whether or not the array is updated at the present cycle. 

\lstset{basicstyle=\scriptsize}
\begin{lstlisting}[language=c++,caption={from \texttt{system.cpp}}]
NEED TO ADD EXACT IMPLEMENTATION
\end{lstlisting}

For profiling, we used \textbf{callgrind} with \textbf{kcachegrind} visualization, which are great tools when we want to find out which functions that steal CPU time.

\section{Readability}
To maximize the readability, we developed a highly object oriented code based on the theory in chapter \eqref{chp:scientificprogramming}. For instance, each wave function element was treated as an object, with the properties \texttt{updateArrays}, \texttt{setArrays}, \texttt{resetArrays}, \texttt{initializeArrays}, \texttt{updateParameters}, \texttt{evaluateRatio}, \texttt{computeGradient}, \texttt{computeLaplacian} and \newline\texttt{computeParameterGradient}. To ensure that all wave function elements have all the necessary properties, the super class \texttt{WaveFunctions} is equipped with the corresponding virtual functions
\lstset{basicstyle=\scriptsize}
\begin{lstlisting}[language=c++,caption={\texttt{wavefunction.h}}]
#pragma once
#include <Eigen/Dense>
#include <iostream>

class WaveFunction {
public:
WaveFunction(class System *system);
virtual void   updateArrays    (Eigen::VectorXd positions, int pRand)  = 0;
virtual void   setArrays       () = 0;
virtual void   resetArrays     () = 0;
virtual void   initializeArrays(Eigen::VectorXd positions) = 0;
virtual void   updateParameters(Eigen::MatrixXd parameters, int elementNumber) = 0;

virtual double evaluateRatio   ()      = 0;
virtual double computeGradient (int k) = 0;
virtual double computeLaplacian()      = 0;

virtual Eigen::VectorXd computeParameterGradient() = 0;

virtual ~WaveFunction() = 0;

protected:
int     m_numberOfParticles                 = 0;
int     m_numberOfDimensions                = 0;
int     m_numberOfFreeDimensions            = 0;
int     m_maxNumberOfParametersPerElement   = 0;
class System* m_system = nullptr;
};
\end{lstlisting}
which serves a template for all the sub classes (wave function elements). As you might notice, we use the \textbf{lowerCamelCase} naming convention for function and variable names, which means that each word begins with a capital letter except the initial word. For classes, we use the \textbf{UpperCamelCase} to distinguish from function names. This is known to be easy to read, and apart from for example the popular \textbf{snake\_case}, we do not need delimiters between the words, which saves some space. After the naming convention is decided, we are still responsible for giving reasonable names, which is not always an easy task, as Phil Karlton points out. When one sees the name, one should know exactly what the variable/function/class is or does. More about naming conventions can be read at \cite{noauthor_naming_2019}. 

\subsection{Energy calculation}
The way we calculate the total kinetic energy then is based on the theory presented in chapter \ref{chp:WFE}, where we explain that
\begin{equation}
T=-\frac{1}{2}\frac{1}{\Psi_T}\nabla_k^2\Psi_T=-\frac{1}{2}\bigg[\sum_{i=1}^p\nabla_k^2\ln\phi_i + \Big(\sum_{i=1}^p\nabla_k\ln\phi_i\Big)^2\bigg].
\end{equation}
The corresponding implementation thus reads
\lstset{basicstyle=\scriptsize}
\begin{lstlisting}[language=c++,caption={from \texttt{system.cpp}}]
double System::getKineticEnergy() {
    double kineticEnergy = 0;
    for(auto& i : m_waveFunctionElements) {
        kineticEnergy += i->computeLaplacian();
    }
    for(int k = 0; k < m_numberOfFreeDimensions; k++) {
        double nablaLnPsi = 0;
        for(auto& i : m_waveFunctionElements) {
            nablaLnPsi += i->computeGradient(k);
        }
        kineticEnergy += nablaLnPsi * nablaLnPsi;
    }
    return - 0.5 * kineticEnergy;
}
\end{lstlisting}

\subsection{Probability ratio calculation}
In the same chapter we state the obvious fact that 
\begin{equation*}
	\frac{\Psi_T^{\text{new}}}{\Psi_T^{\text{old}}}=\prod_{i=1}^p\frac{\phi_i^{\text{new}}}{\phi_i^{\text{old}}},
\end{equation*}
which can easily be implemented as
\lstset{basicstyle=\scriptsize}
\begin{lstlisting}[language=c++,caption={from \texttt{system.cpp}}]
double System::evaluateWaveFunctionRatio() {
    double ratio = 1;
    for(auto& i : m_waveFunctionElements) {
        ratio *= i->evaluateRatio();
    }
    return ratio;
}
\end{lstlisting}

\subsection{Parameters}
Another consequence of this flexible implementation is that we need to treat all parameters in the same way to make everything general. To do this, we create a global matrix of dimensions $n \cross m$ where $n$ is the number of wave function elements and $m$ is the maximum number of parameters in a wave function element. Thus each element has its own row in the matrix, and one can easily track down a specific parameter. 

For the parameter update, each element needs to provide an array of length $m$ containing its respective parameter gradients. This array is calculated in the function \newline \texttt{computeParameterGradient} for each element, and they are all collected in the function \texttt{getAllInstantGradients}:

\lstset{basicstyle=\scriptsize}
\begin{lstlisting}[language=c++,caption={from \texttt{system.cpp}}]
Eigen::MatrixXd System::getAllInstantGradients() {
    Eigen::MatrixXd gradients = Eigen::MatrixXd::Zero(m_numberOfWaveFunctionElements, \
    m_maxNumberOfParametersPerElement);
    for(int i = 0; i < m_numberOfWaveFunctionElements; i++) {
        gradients.row(i) = m_waveFunctionElements[i]->computeParameterGradient();
    }
    return gradients;
}
\end{lstlisting}
We need to stress that those are the instant gradients calculated every time a particle is moved. Exactly how the parameters are updated depends on an average of these, described in chapter \ref{chp:WFE}. 

Immediately after the parameters are updated, the new parameters need to be sent into the wave function elements. This is done through the functions \texttt{updateParameters}, which update all the parameters and weights in the elements. In addition, the function has the responsibility to order the elements such that none gets the same number. 


\section{Structure} \label{subsec:structure}
How the classes are communicating is no easy task to explain, most classes are calling other classes, there is no tidy way to visualize the actual code flow. However, a simplified structure chart can still be informative, and in figure \eqref{fig:structurechart} the most important calls between the different classes are pointed out. We decided to leave out \texttt{main.cpp} since all it does is to set the different classes. 

\input{tikz/program_structure.tex}

This is the main aim of the flow, but the actual flow does also depend on system. For instance, when using importance sampling, we will have an additional call between \texttt{WaveFunctions} and \texttt{Metropolis} due to calculations of the quantum force. 

\section{Random number generators} \label{sec:RNG}
In the Monte-Carlo sampling we are drawing millions of numbers, and in order to get accurate estimations, they should all be random, independent and fast to get. 

In C++ there are plenty of random number generator available, but not all of them meet our requirements. For instance the standard 

Mersenne Twister needs to be mentioned here

\section{Foundation} \label{subsec:foundation}
The foundation of the code are all the super classes, nine in the number. They all have multiple sub classes, and the reader needs to specify which sub class to be used. The exception is the \texttt{WaveFunctions} class, as described above, where multiple sub classes can be used. Below, the role of all the super classes will be discussed briefly and the difference between various sub classes will be explained. 

\subsection{Super classes}

\subsubsection{The \texttt{Basis} class}
In this class, one needs to choose which basis set that should be used in the Slater determinant. There are three required functions:
\begin{itemize}
	\item \texttt{numberOfOrbitals()} gives the number of orbitals given the number of particles and dimensions. This is used in the Slater determinant.
	
	\item \texttt{evaluate(double x, int n)} gives the the value of element \texttt{n} for a given \texttt{x}. 
	
	\item \texttt{evaluateDerivative(double x, int n)} gives the derivative of element \texttt{n} with respect to \texttt{x} for a given \texttt{x}.  
\end{itemize}

Possible sub classes choices are \texttt{Hermite} and \texttt{HydrogenLike}, where the former is well-suited for quantum dots and the latter is used in atomic structure calculations. 

\subsubsection{The \texttt{Hamiltonians} class}
In this class, one needs to specify the Hamiltonian of the system. The only required function is \texttt{computeLocalEnergy()}, which returns the local energy. One can choose between the Hamiltonians \texttt{AtomicNucleus} and \texttt{HarmonicOscillator}, where the first one sets up an external potential like the one we find in an atom, and takes the atomic number $Z$ as an argument. The second one sets up a harmonic oscillator potential, and actually the only thing that distinguish the who classes is the external energy calculation. 

\subsubsection{The \texttt{InitialStates} class}
In one way or another we need to initialize the particle positions, but how we want to do this depends on the situation. The implemented methods are randomly initialized positions drawn from a uniform or normal distribution, \texttt{RandomUniform} and \texttt{RandomNormal} respectively. They consist of the function \texttt{setupInitialState()}.

\subsubsection{The \texttt{InitialWeights} class}
In the same manner as the \texttt{InitialStates} class, we can initialize the weights in various ways. One way is to set all the weights to the same initial value, represented by the sub class \texttt{Constant}. It takes an argument \texttt{factor} which gives the initial value of all weights.

A second choice is random initial weights, where the class \texttt{Randomize} initializes the weights based on a uniform distribution. Also this class takes the \texttt{factor} argument, which defines the interval. By default, the interval is [-1,1], which corresponds to \texttt{factor=1}.

\subsubsection{The \texttt{Metropolis} class}
This class is the true sampling class, where the magic sampling is done. Three sampling methods are implemented:
\begin{itemize}
	\item \texttt{BruteForce} is the standard Metropolis sampling, where a particle is moved in a totally random direction and the move is accepted if the new probability is high enough.

	\item \texttt{ImportanceSampling} is a more advanced version of the Metropolis algorithm, where the particle is moved in the same direction as the quantum force.
	
	\item \texttt{GibbsSampling} is not directly related to the Metropolis algorithm, it is a simple method which is widely used in Boltzmann machines.
\end{itemize}
The sub classes need to have the function \texttt{acceptMove()}, where the particle is moved and the the move is either accepted or rejected. To get the new positions, one need to call \texttt{updatePositions()}. which is member of the super class. 

\subsubsection{The \texttt{Optimization} class}
The next class is the \texttt{Optimization} class, where the weight update is performed in the function \texttt{updateWeights()}. Also the instant gradients (the gradient for each step) is calculated here, in the function \texttt{getAllInstantGradients()}.

Two gradient based stochastic methods are implemented: \texttt{StochasticGradientDescent} and \texttt{ADAM}, with descriptive names. They both takes an argument \texttt{gamma} which is the prefactor in front of the momentum. The reader can consult chapter \eqref{chp:optimization} for details on how the optimization methods work. 

\subsubsection{The \texttt{Plotter} class}
Not sure if I will keep this as a class

\subsubsection{The \texttt{RNG} class}
The random number generator (RNG) was implemented as a class to ease the switch between different RNGs. Each subclass need to contain the following functions:
\begin{itemize}
	\item \texttt{nextInt(int upperLimit)} returns the next number in the RNG sequence as an integer between 0 and \texttt{upperLimit}.
	
	\item \texttt{nextDouble()} returns the next number in the RNG sequence as a double between 0 and 1.
	
	\item \texttt{nextGaussian(double mean, double standardDeviation)} returns the next number in the RNG sequence, regenerated by a normal distribution with mean value \texttt{mean} and standard deviation \texttt{standardDeviation}.
\end{itemize}
The two available RNGs are the Mersenne Twister number generator, \texttt{MersenneTwister} and... . For the theory behind thoe methods, see section \eqref{sec:RNG}. 

\subsubsection{The \texttt{WaveFunctions} class}
Last, but not least, the \texttt{WaveFunctions} class contains all the wave function related computations. We have already mentioned it, but all the details are still to be stressed. 

The required functions in the wave function elements are
\begin{itemize}
	\item \texttt{updateArrays(Eigen::VectorXd positions, int pRand)} which update position dependent arrays recursively with respect to the new positions, \texttt{positions} and the changed position index \texttt{pRand}. 
	
	\item \texttt{resetArrays()} set the arrays back to the old values when a move is rejected.
	
	\item \texttt{initializeArrays(Eigen::VectorXd positions)} initialize all arrays at the beginning. This is the only moment when the arrays cannot be updated recursively. 
	
	\item \texttt{updateParameters(Eigen::MatrixXd parameters, int elementNumber)} updates the weights. All weights of the system are stored in the parent matrix \texttt{parameters}, while each wave function element has child weight matrices and arrays which are mapped from the parent. They are all updated in this function. \texttt{elementNumber} is the number of the element, and is unique for all the wave function elements.
	
	\item \texttt{evaluateRatio()} returns the ratio between the new and the old probability, \\ $|\Psi_T(\bs{r}_{\text{new}})|^2/|\Psi_T(\bs{r}_{\text{old}})|^2$
	
	\item \texttt{computeFirstDerivative(int k)} returns the first derivative of the wave function element with respect to the position index \texttt{k}.
	
	\item \texttt{computeSecondDerivative()} returns the second derivative of the wave function element with respect to all position indices. 
	
	\item \texttt{computeFirstEnergyDerivative(int k)} returns the derivative of the position \texttt{k} first derivative of the wave function element with respect to all the weights, $\partial/\partial \alpha_i \nabla_k ln(\psi)$. The outcome is an array.
	
	\item \texttt{computeSecondEnergyDerivative()} returns the derivative of the position second derivative of the wave function element with respect to all the weights, $\sum_k\partial/\partial \alpha_i \nabla_k^2 ln(\psi)$. The outcome is an array.
\end{itemize}
The wave function elements implemented are 
\begin{itemize}
	\item \texttt{Gaussian} is the simple Gaussian function.
	\item \texttt{PadeJastrow} is the Padé-Jastrow factor.
	\item \texttt{SlaterDeterminant} is the Slater determinant.
	\item \texttt{MLGaussian} is the Gaussian part derived from the Boltzmann machines.
	\item \texttt{NQSJastrow} is the product part derived from the Boltzmann machines. 
\end{itemize}

New wave function elements can easily be implemented, all one needs to do is to calculate all the derivatives and specify how to update the position dependent arrays recursively. 

\subsection{How to set sub classes?}
We have now described all the available super classes and sub classes, but how do we set them? As hinted in the beginning of the chapter, the entire system should be specified in \texttt{main.cpp}. For example, a harmonic oscillator Hamiltonian can be set by
\begin{lstlisting}[language=c++]
system->setHamiltonian(new HarmonicOscillator(system));
\end{lstlisting}
which is calling the function \texttt{setHamiltonian} in the class \texttt{System}. This function sets the official Hamiltonian object to \texttt{HarmonicOscillator}, such that every time we call the super class \texttt{Hamiltonian}, we are forwarded to \texttt{HarmonicOscillator}. The \texttt{System} class is basically filled with functions that set objects and scalars. To make those objects and scalars available in other classes, the \texttt{System} header is equipped with get-functions. For instance, there exist a function 
\begin{lstlisting}
class Hamiltonian* getHamiltonian() { return m_hamiltonian; }
\end{lstlisting}
which returns the correct Hamiltonian sub class. In the other classes where the \texttt{System} objects appears as \texttt{m\_system}, the local energy can be found by
\begin{lstlisting}
double localEnergy = m_system->getHamiltonian->computeLocalEnergy();
\end{lstlisting}
Similar functions exist for other essential objects, arrays and scalar. 
